{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f2c341c3670>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "import promptsource.templates\n",
    "import numpy.typing as npt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from typing import Dict, Any, List, Union\n",
    "\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset winogrande (/root/.cache/huggingface/datasets/winogrande/winogrande_xs/1.1.0/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab5f28fd3c954a598a6b65a6d9687605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "remove_sos_token = False\n",
    "token_pos = -1\n",
    "\n",
    "dataset_name = \"winogrande\"\n",
    "dataset_config = \"winogrande_xs\"\n",
    "dataset_split = \"validation\"\n",
    "prompt_template = \"fill in the blank\"\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "def create_prompt(example: Dict[str, Any]) -> str:\n",
    "    return template.apply(example)[0]\n",
    "\n",
    "\n",
    "templates = promptsource.templates.DatasetTemplates(\n",
    "    dataset_name, dataset_config\n",
    ")\n",
    "template = templates[prompt_template]\n",
    "dataset = datasets.load_dataset(dataset_name, dataset_config)\n",
    "assert isinstance(dataset, datasets.dataset_dict.DatasetDict)\n",
    "dataset = dataset[dataset_split]\n",
    "prompt_creator = create_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.device(device):\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=\"auto\",\n",
    "        # torch_dtype=torch.float,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1267/1267 [02:38<00:00,  7.98it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.device(device):\n",
    "    all_representations = []\n",
    "    prompts = list(map(prompt_creator, dataset))  # type:ignore\n",
    "    for prompt in tqdm(prompts):\n",
    "        toks = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        input_ids = toks[\"input_ids\"].to(torch.device(device))  # type:ignore\n",
    "        out = model(\n",
    "            input_ids=input_ids, output_hidden_states=True\n",
    "        ).hidden_states  # Tuple with elements shape(1, n_tokens, dim)\n",
    "        assert isinstance(out[0], torch.Tensor)\n",
    "        # Some models have the representations of special start-of-sentence tokens.\n",
    "        # We typically do not care about those.\n",
    "        if remove_sos_token:\n",
    "            out = tuple((representations[:, 1:, :] for representations in out))\n",
    "        out = tuple((representations.to(\"cpu\") for representations in out))\n",
    "\n",
    "        all_representations.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-2.7103e-03, -6.1737e-02, -6.5063e-02,  ..., -6.8963e-05,\n",
       "            3.5461e-02,  3.5919e-02],\n",
       "          [ 5.1918e-03, -1.5465e-02, -4.8180e-03,  ..., -1.8982e-02,\n",
       "            3.5309e-02, -2.7817e-02],\n",
       "          [ 3.0766e-03,  1.0941e-02,  1.9121e-04,  ...,  2.8191e-03,\n",
       "            6.4453e-02, -1.6129e-02],\n",
       "          ...,\n",
       "          [ 7.9155e-04, -6.3705e-04, -1.1139e-03,  ..., -3.6144e-03,\n",
       "            8.0505e-02, -1.0635e-02],\n",
       "          [-1.6052e-02, -1.7929e-02, -4.4281e-02,  ..., -2.3865e-02,\n",
       "            4.3915e-02, -1.0042e-03],\n",
       "          [-1.2383e-02, -1.4671e-02,  1.4946e-02,  ..., -2.0737e-02,\n",
       "           -5.3101e-02, -1.8280e-02]]], dtype=torch.float16),\n",
       " tensor([[[ 1.0469,  0.3872, -0.1362,  ...,  0.4907,  0.0220, -0.9336],\n",
       "          [ 0.5278, -0.2140,  0.2418,  ..., -0.0556, -0.0890, -0.5308],\n",
       "          [-0.1445, -0.1400, -0.1644,  ..., -0.0718,  0.0792, -0.3521],\n",
       "          ...,\n",
       "          [-0.2489,  0.1942,  0.0184,  ..., -0.1066,  0.0500,  0.0839],\n",
       "          [ 0.0491,  0.0822, -0.0224,  ...,  0.0937,  0.0089, -0.2283],\n",
       "          [ 0.2428, -0.1543,  0.2488,  ..., -0.3196,  0.0202, -0.2052]]],\n",
       "        dtype=torch.float16),\n",
       " tensor([[[ 0.7764,  0.8965,  0.4492,  ...,  1.1055,  0.8169, -0.4570],\n",
       "          [ 0.4043, -0.0917,  0.0015,  ...,  0.5352,  1.0371,  0.0591],\n",
       "          [ 0.2593,  0.2651, -0.2437,  ...,  0.1946,  0.5562, -0.2078],\n",
       "          ...,\n",
       "          [-0.0861, -0.1971, -0.0143,  ...,  0.1186, -0.0786, -0.2316],\n",
       "          [ 0.2820, -0.4719,  0.0241,  ...,  0.1824,  0.1727, -0.2715],\n",
       "          [ 0.5146, -0.7144,  0.5869,  ..., -0.3948,  0.4121,  0.0507]]],\n",
       "        dtype=torch.float16),\n",
       " tensor([[[ 0.9805, -0.0605,  0.5303,  ...,  2.1523,  0.6240, -0.3872],\n",
       "          [ 0.1099, -0.4536,  0.2367,  ...,  1.0400,  1.4150, -0.5562],\n",
       "          [-0.0522, -0.3398,  0.1909,  ...,  0.7363,  0.8804, -0.7617],\n",
       "          ...,\n",
       "          [-0.0201,  0.3823, -0.6758,  ...,  0.5610, -0.5186, -0.1271],\n",
       "          [ 0.2610, -0.4766, -0.1398,  ...,  0.1262, -0.2391, -0.3630],\n",
       "          [ 0.6499, -0.5923,  0.7334,  ..., -0.7168,  0.6299, -0.0999]]],\n",
       "        dtype=torch.float16),\n",
       " tensor([[[ 4.6836e+00, -5.2734e-02, -7.6074e-01,  ...,  2.4688e+00,\n",
       "            1.6338e+00,  5.2344e-01],\n",
       "          [ 1.0215e+00, -6.7773e-01, -2.5146e-01,  ...,  7.9004e-01,\n",
       "            7.4805e-01,  1.6357e-01],\n",
       "          [ 1.2031e+00, -2.9590e-01, -2.8857e-01,  ...,  4.8633e-01,\n",
       "            2.3535e-01, -1.6650e-01],\n",
       "          ...,\n",
       "          [ 1.0459e+00,  2.0483e-01, -6.8213e-01,  ...,  3.4082e-01,\n",
       "            9.7656e-04, -1.1884e-01],\n",
       "          [ 3.2251e-01, -5.8643e-01, -8.1396e-01,  ...,  4.2700e-01,\n",
       "            1.8250e-01, -5.5615e-01],\n",
       "          [ 1.2344e+00, -5.3271e-01, -2.0996e-02,  ...,  3.2520e-01,\n",
       "            4.8633e-01, -1.9067e-01]]], dtype=torch.float16),\n",
       " tensor([[[-0.2188,  0.1733, -0.2854,  ...,  1.7207,  5.9727,  1.0498],\n",
       "          [-0.2412, -0.2954, -0.3911,  ...,  0.3767,  1.6758,  0.1909],\n",
       "          [-0.2344, -0.6562, -0.2405,  ...,  0.0330,  1.4150,  0.0996],\n",
       "          ...,\n",
       "          [-0.4326, -0.1721, -0.8149,  ...,  0.4351,  1.6719,  0.1628],\n",
       "          [-0.0637, -0.5869, -0.4485,  ...,  0.4492,  1.2812,  0.0288],\n",
       "          [ 0.3740, -0.3428, -0.4360,  ...,  0.0851,  1.2061,  0.0771]]],\n",
       "        dtype=torch.float16),\n",
       " tensor([[[25.3281, 12.7812, 13.6797,  ..., 18.8750, 15.1641, 19.6562],\n",
       "          [26.4688, 13.0000, 16.1250,  ..., 17.1094, 15.3672, 20.9219],\n",
       "          [26.9062, 12.0547, 14.7969,  ..., 16.9219, 13.0234, 20.7969],\n",
       "          ...,\n",
       "          [24.7500, 10.6719, 14.0312,  ..., 15.8828, 17.1250, 19.6875],\n",
       "          [25.0781, 12.2969, 15.3125,  ..., 16.6875, 15.4297, 19.6719],\n",
       "          [26.7500, 13.8047, 14.9141,  ..., 15.1250, 13.8828, 20.4375]]],\n",
       "        dtype=torch.float16))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9805, -0.0605,  0.5303,  ...,  2.1523,  0.6240, -0.3872],\n",
       "        [ 0.1099, -0.4536,  0.2367,  ...,  1.0400,  1.4150, -0.5562],\n",
       "        [-0.0522, -0.3398,  0.1909,  ...,  0.7363,  0.8804, -0.7617],\n",
       "        ...,\n",
       "        [-0.0394,  0.1534, -0.4124,  ...,  0.6953, -0.6968,  0.2260],\n",
       "        [ 0.2607, -0.6787, -0.0059,  ...,  0.2876, -0.5625, -0.0554],\n",
       "        [ 0.6807, -0.7148,  0.9131,  ..., -0.6611,  0.4644,  0.1693]],\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_representations[0][3][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9805, -0.0605,  0.5303,  ...,  2.1523,  0.6240, -0.3872],\n",
       "        [ 0.1099, -0.4536,  0.2367,  ...,  1.0400,  1.4150, -0.5562],\n",
       "        [-0.0522, -0.3398,  0.1909,  ...,  0.7363,  0.8804, -0.7617],\n",
       "        ...,\n",
       "        [-0.2395,  0.3196, -0.4863,  ...,  0.6670, -0.3704, -0.0927],\n",
       "        [ 0.0042, -0.5381, -0.1251,  ...,  0.3706, -0.2317, -0.2263],\n",
       "        [ 0.4961, -0.6665,  0.7998,  ..., -0.6533,  0.7349, -0.0464]],\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_representations[9][3][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 41, 512]) torch.Size([1, 47, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1.3770e-01, -5.1270e-02,  1.7139e-01, -4.3213e-02, -5.8105e-02,\n",
       "         2.3975e-01,  1.0669e-01,  1.7334e-02, -7.8613e-02,  1.2988e-01,\n",
       "        -8.5083e-02,  4.4006e-02,  1.9531e-01,  9.9487e-02, -1.6846e-01,\n",
       "         4.0527e-02, -6.5918e-02, -2.0020e-02,  6.2256e-03, -2.5635e-02,\n",
       "        -5.8472e-02,  3.7012e-01,  4.1504e-03, -7.8125e-03,  9.2285e-02,\n",
       "        -4.9194e-02, -4.9438e-02,  1.2793e-01,  1.4807e-01, -1.3745e-01,\n",
       "        -4.6875e-02, -1.4868e-01, -8.7280e-02, -3.2959e-03,  1.7261e-01,\n",
       "        -2.1729e-02, -5.5664e-02,  2.8076e-02, -1.0620e-02,  1.5015e-02,\n",
       "         1.0986e-01, -8.3984e-02,  3.2379e-02, -5.5298e-02, -1.3428e-02,\n",
       "         2.4902e-02,  9.7168e-02, -5.4297e-01, -2.2754e-01, -1.0229e-01,\n",
       "         6.8604e-02,  9.5947e-02, -1.8408e-01,  8.7646e-02,  3.3691e-02,\n",
       "         1.2183e-01,  1.0938e-01,  6.8359e-02,  4.5410e-02,  4.1748e-02,\n",
       "         1.8909e-01, -5.9082e-02, -2.3804e-02,  5.6152e-02, -1.1865e-01,\n",
       "         2.4841e-02,  7.1533e-02, -6.5430e-02, -1.9580e-01, -5.8594e-03,\n",
       "        -1.3965e-01,  1.3428e-02, -1.5186e-01, -1.0596e-01, -2.4927e-01,\n",
       "        -2.4414e-03, -1.8555e-02, -4.2480e-02,  7.1289e-02,  7.4951e-02,\n",
       "        -4.3213e-02, -1.4648e-03,  1.4062e-01, -8.3984e-02, -1.9775e-02,\n",
       "         4.0039e-02, -6.0059e-02, -1.9580e-01, -1.2939e-02, -1.6162e-01,\n",
       "        -1.1230e-01,  2.0581e-01,  9.7656e-02, -9.5642e-02,  2.3022e-01,\n",
       "         1.5479e-01,  3.7598e-02,  1.2634e-01,  1.6699e-01, -1.1932e-01,\n",
       "         1.4893e-01, -2.1814e-01,  1.0858e-01,  3.4668e-02, -8.3008e-02,\n",
       "         1.4294e-01,  1.7627e-01,  2.0142e-02,  3.5156e-02,  1.6772e-01,\n",
       "         3.7598e-02, -1.1328e-01, -1.0205e-01,  1.2878e-01, -1.4087e-01,\n",
       "        -1.4392e-01, -9.1553e-04, -2.2217e-01,  2.8076e-01, -1.9897e-02,\n",
       "        -1.5625e-02, -2.9785e-02, -1.0266e-01,  1.0974e-01, -5.9814e-02,\n",
       "         3.6621e-02,  8.1787e-02, -4.1016e-02, -1.0742e-01, -1.3489e-01,\n",
       "        -4.1016e-02,  3.2959e-02,  5.1758e-02, -6.1035e-03,  2.3877e-01,\n",
       "        -1.9214e-01, -4.2969e-02,  1.6602e-02, -4.4922e-02,  1.3403e-01,\n",
       "        -2.4414e-04, -5.5908e-02,  1.1060e-01,  2.3926e-02,  3.9429e-02,\n",
       "         1.6846e-02, -6.5186e-02,  1.3574e-01, -6.4453e-02, -1.0474e-01,\n",
       "        -1.2695e-02,  3.4180e-03, -1.0205e-01, -3.0981e-01, -1.1401e-01,\n",
       "         1.9312e-01,  1.6406e-01, -2.0410e-01,  6.3049e-02, -5.8105e-02,\n",
       "         1.2158e-01, -1.6187e-01,  6.5796e-02, -2.0752e-02,  9.1553e-02,\n",
       "         1.2793e-01, -1.2964e-01,  6.6162e-02, -8.5449e-03, -3.6255e-02,\n",
       "         3.7598e-02, -7.5684e-02,  5.5420e-02,  6.0791e-02, -1.6602e-01,\n",
       "         5.7373e-02,  1.2891e-01,  1.2128e-01,  2.8931e-01, -1.3672e-02,\n",
       "        -1.0791e-01,  2.3376e-02, -5.9082e-02,  6.1768e-02,  8.1055e-02,\n",
       "         4.2480e-02, -1.7090e-01, -5.9570e-02, -6.1523e-02,  4.7974e-02,\n",
       "         4.3488e-02,  2.0752e-01,  3.9258e-01, -6.2500e-02, -1.6357e-02,\n",
       "        -1.5051e-01,  1.6406e-01, -1.6748e-01, -1.5381e-01, -8.1177e-02,\n",
       "        -2.0264e-02,  4.1016e-02, -4.8828e-04,  2.4902e-02,  5.2368e-02,\n",
       "        -5.0781e-02, -1.8872e-01, -1.5503e-01, -9.6191e-02,  7.1289e-02,\n",
       "         2.1313e-01,  2.5146e-02, -1.2445e-01,  1.6602e-01, -2.6855e-02,\n",
       "         1.3611e-02, -1.5625e-02, -2.0508e-02, -3.3569e-02, -1.6809e-01,\n",
       "        -1.9287e-01, -1.1304e-01,  2.5574e-02, -2.6489e-01,  1.0425e-01,\n",
       "         7.2510e-02, -2.3926e-02, -1.3574e-01,  1.7334e-02,  3.5889e-02,\n",
       "         1.6064e-01,  4.0039e-02, -2.2461e-02, -5.2734e-02, -1.3770e-01,\n",
       "         7.2510e-02,  3.4180e-02,  8.5571e-02, -3.4912e-02,  5.7495e-02,\n",
       "        -4.8584e-02,  2.2119e-01, -1.1279e-01,  1.5381e-02, -1.5283e-01,\n",
       "        -7.9834e-02,  0.0000e+00, -8.4961e-02, -2.5391e-02,  5.0293e-02,\n",
       "        -5.8838e-02, -1.0596e-01,  9.4238e-02, -7.8125e-02,  4.1870e-02,\n",
       "        -1.3428e-03,  8.1055e-02,  1.8677e-01, -3.0273e-02, -1.7090e-01,\n",
       "        -1.3428e-01,  6.7139e-02, -3.1250e-01, -3.5645e-01, -9.5459e-02,\n",
       "        -9.0637e-02, -5.3711e-02, -6.1035e-03, -9.3384e-02,  1.0083e-01,\n",
       "         8.4167e-02,  2.3145e-01,  2.2998e-01,  2.8809e-02,  8.3496e-02,\n",
       "         6.4087e-02,  1.6211e-01, -7.8125e-02,  5.3955e-02,  2.3315e-02,\n",
       "        -5.3711e-03,  6.3599e-02, -2.6611e-02, -2.3975e-01,  3.9062e-03,\n",
       "        -5.6641e-02, -1.8091e-01, -3.9062e-02,  1.3452e-01,  5.2612e-02,\n",
       "        -8.4534e-02,  3.3496e-01,  6.2500e-02,  4.9194e-02,  9.1553e-03,\n",
       "         2.0508e-02,  1.1426e-01, -5.6366e-02, -1.8555e-02,  1.1499e-01,\n",
       "         3.7781e-02,  6.5430e-02,  2.6318e-01,  3.0273e-02,  6.0303e-02,\n",
       "        -1.6138e-01,  1.6589e-01, -1.7773e-01,  5.7373e-02,  1.2598e-01,\n",
       "         5.7983e-02,  2.4414e-04, -1.3403e-01,  1.4600e-01,  3.1494e-02,\n",
       "        -1.2598e-01, -2.7344e-02,  7.5195e-02,  7.5684e-02, -5.0781e-02,\n",
       "         2.0996e-02, -1.2866e-01, -1.5283e-01, -1.1816e-01,  1.3098e-01,\n",
       "         6.7871e-02,  1.8555e-01,  1.7261e-01,  1.6553e-01,  2.0020e-02,\n",
       "        -1.6956e-01, -7.4219e-02, -8.1055e-02, -2.5391e-02,  6.3477e-02,\n",
       "        -9.3262e-02, -2.5909e-02,  1.6113e-02,  1.1426e-01, -1.1328e-01,\n",
       "         6.5186e-02, -1.0303e-01,  1.2939e-02, -2.1362e-01,  1.2378e-01,\n",
       "        -1.2402e-01, -1.4648e-02, -6.3232e-02, -9.6924e-02, -1.0059e-01,\n",
       "         2.9297e-03,  5.6396e-02, -3.1555e-02,  1.1169e-01, -8.7891e-03,\n",
       "         4.5166e-02,  5.0293e-02,  9.0820e-02,  1.2988e-01, -3.4668e-02,\n",
       "         4.8462e-02,  1.8066e-02, -1.6602e-02,  2.9541e-02,  4.1260e-02,\n",
       "         1.6821e-01,  1.1877e-01,  5.2429e-02, -3.4155e-01, -3.7158e-01,\n",
       "        -7.3975e-02, -1.9006e-01, -1.9043e-01,  4.9805e-02,  4.2236e-02,\n",
       "        -1.4893e-01,  1.2646e-01,  7.2266e-02,  1.1230e-02, -7.6660e-02,\n",
       "        -3.7109e-02,  1.1816e-01, -3.4912e-02,  2.6855e-02,  1.8213e-01,\n",
       "        -1.2720e-01, -4.4922e-02,  7.5195e-02,  5.7373e-02, -2.0312e-01,\n",
       "         1.0559e-01, -5.5176e-02, -7.6172e-02, -1.6357e-02, -2.0129e-01,\n",
       "         1.2500e-01,  9.9609e-02,  5.6152e-03, -1.1426e-01, -2.5537e-01,\n",
       "        -5.3345e-02,  2.5391e-02, -1.9043e-02,  1.9409e-01, -7.8369e-02,\n",
       "        -3.4912e-02,  8.5938e-02,  9.2651e-02,  7.7637e-02, -1.1987e-01,\n",
       "         9.4238e-02, -8.6426e-02, -5.3467e-02, -1.6016e-01,  1.8359e-01,\n",
       "         9.2163e-02, -6.2988e-02,  4.1748e-02, -6.3477e-03,  1.6113e-02,\n",
       "         6.9336e-02, -2.6318e-01,  2.0508e-02, -1.7285e-01,  4.5410e-02,\n",
       "        -1.1707e-01, -1.1719e-01,  2.0264e-02, -1.7212e-02, -6.3965e-02,\n",
       "         8.3252e-02, -1.5918e-01,  1.9775e-02,  2.0898e-01,  6.9092e-02,\n",
       "         4.8828e-02, -1.6211e-01, -7.0435e-02,  6.9214e-02,  1.0425e-01,\n",
       "         3.2593e-02, -8.1055e-02,  9.4604e-02, -8.6792e-02,  6.4941e-02,\n",
       "        -1.4844e-01,  1.6357e-02, -1.3159e-01,  9.2285e-02, -8.3435e-02,\n",
       "        -2.9541e-02,  6.2866e-02, -1.7432e-01,  1.0547e-01,  9.3994e-02,\n",
       "        -8.8867e-02, -4.1016e-02, -5.8594e-03,  5.9570e-02, -7.0923e-02,\n",
       "        -3.9062e-03, -2.2510e-01,  1.6968e-02, -9.0332e-02,  7.7148e-02,\n",
       "         3.9062e-03, -1.9360e-01, -6.4697e-03,  2.7588e-02,  7.4219e-02,\n",
       "        -9.7656e-02, -7.6172e-02,  2.2510e-01,  1.9434e-01, -4.8340e-02,\n",
       "         1.1963e-01,  1.0376e-03, -2.3438e-02, -5.2734e-02,  3.4302e-02,\n",
       "         1.1377e-01, -9.8389e-02,  2.1973e-02,  1.2695e-02,  5.1270e-02,\n",
       "         1.4697e-01,  5.7739e-02, -1.5308e-01, -1.6846e-01, -3.2227e-02,\n",
       "        -1.1719e-01,  4.0771e-02,  6.1279e-02,  6.1340e-02,  8.2520e-02,\n",
       "        -4.8828e-04,  3.4973e-02, -7.8613e-02, -2.2766e-02,  9.0149e-02,\n",
       "        -1.2012e-01,  5.4688e-02,  1.9409e-02, -4.5288e-02, -4.2969e-02,\n",
       "         1.3770e-01,  3.1738e-02,  2.2778e-01,  3.9062e-02,  1.2305e-01,\n",
       "        -2.4414e-04,  2.4670e-01], dtype=torch.float16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = all_representations[0][3]\n",
    "r2 = all_representations[3][3]\n",
    "\n",
    "print(r1.size(), r2.size())\n",
    "((r1[0, -1, :] - r2[0, -1, :]) == 0).sum()\n",
    "r1[0, -1, :] - r2[0, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps_per_input = all_representations\n",
    "r = torch.cat(\n",
    "    [torch.stack(reps, dim=0)[:, :, token_pos, :] for reps in reps_per_input],\n",
    "    dim=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512]) torch.Size([512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1.3770e-01, -5.1270e-02,  1.7139e-01, -4.3213e-02, -5.8105e-02,\n",
       "         2.3975e-01,  1.0669e-01,  1.7334e-02, -7.8613e-02,  1.2988e-01,\n",
       "        -8.5083e-02,  4.4006e-02,  1.9531e-01,  9.9487e-02, -1.6846e-01,\n",
       "         4.0527e-02, -6.5918e-02, -2.0020e-02,  6.2256e-03, -2.5635e-02,\n",
       "        -5.8472e-02,  3.7012e-01,  4.1504e-03, -7.8125e-03,  9.2285e-02,\n",
       "        -4.9194e-02, -4.9438e-02,  1.2793e-01,  1.4807e-01, -1.3745e-01,\n",
       "        -4.6875e-02, -1.4868e-01, -8.7280e-02, -3.2959e-03,  1.7261e-01,\n",
       "        -2.1729e-02, -5.5664e-02,  2.8076e-02, -1.0620e-02,  1.5015e-02,\n",
       "         1.0986e-01, -8.3984e-02,  3.2379e-02, -5.5298e-02, -1.3428e-02,\n",
       "         2.4902e-02,  9.7168e-02, -5.4297e-01, -2.2754e-01, -1.0229e-01,\n",
       "         6.8604e-02,  9.5947e-02, -1.8408e-01,  8.7646e-02,  3.3691e-02,\n",
       "         1.2183e-01,  1.0938e-01,  6.8359e-02,  4.5410e-02,  4.1748e-02,\n",
       "         1.8909e-01, -5.9082e-02, -2.3804e-02,  5.6152e-02, -1.1865e-01,\n",
       "         2.4841e-02,  7.1533e-02, -6.5430e-02, -1.9580e-01, -5.8594e-03,\n",
       "        -1.3965e-01,  1.3428e-02, -1.5186e-01, -1.0596e-01, -2.4927e-01,\n",
       "        -2.4414e-03, -1.8555e-02, -4.2480e-02,  7.1289e-02,  7.4951e-02,\n",
       "        -4.3213e-02, -1.4648e-03,  1.4062e-01, -8.3984e-02, -1.9775e-02,\n",
       "         4.0039e-02, -6.0059e-02, -1.9580e-01, -1.2939e-02, -1.6162e-01,\n",
       "        -1.1230e-01,  2.0581e-01,  9.7656e-02, -9.5642e-02,  2.3022e-01,\n",
       "         1.5479e-01,  3.7598e-02,  1.2634e-01,  1.6699e-01, -1.1932e-01,\n",
       "         1.4893e-01, -2.1814e-01,  1.0858e-01,  3.4668e-02, -8.3008e-02,\n",
       "         1.4294e-01,  1.7627e-01,  2.0142e-02,  3.5156e-02,  1.6772e-01,\n",
       "         3.7598e-02, -1.1328e-01, -1.0205e-01,  1.2878e-01, -1.4087e-01,\n",
       "        -1.4392e-01, -9.1553e-04, -2.2217e-01,  2.8076e-01, -1.9897e-02,\n",
       "        -1.5625e-02, -2.9785e-02, -1.0266e-01,  1.0974e-01, -5.9814e-02,\n",
       "         3.6621e-02,  8.1787e-02, -4.1016e-02, -1.0742e-01, -1.3489e-01,\n",
       "        -4.1016e-02,  3.2959e-02,  5.1758e-02, -6.1035e-03,  2.3877e-01,\n",
       "        -1.9214e-01, -4.2969e-02,  1.6602e-02, -4.4922e-02,  1.3403e-01,\n",
       "        -2.4414e-04, -5.5908e-02,  1.1060e-01,  2.3926e-02,  3.9429e-02,\n",
       "         1.6846e-02, -6.5186e-02,  1.3574e-01, -6.4453e-02, -1.0474e-01,\n",
       "        -1.2695e-02,  3.4180e-03, -1.0205e-01, -3.0981e-01, -1.1401e-01,\n",
       "         1.9312e-01,  1.6406e-01, -2.0410e-01,  6.3049e-02, -5.8105e-02,\n",
       "         1.2158e-01, -1.6187e-01,  6.5796e-02, -2.0752e-02,  9.1553e-02,\n",
       "         1.2793e-01, -1.2964e-01,  6.6162e-02, -8.5449e-03, -3.6255e-02,\n",
       "         3.7598e-02, -7.5684e-02,  5.5420e-02,  6.0791e-02, -1.6602e-01,\n",
       "         5.7373e-02,  1.2891e-01,  1.2128e-01,  2.8931e-01, -1.3672e-02,\n",
       "        -1.0791e-01,  2.3376e-02, -5.9082e-02,  6.1768e-02,  8.1055e-02,\n",
       "         4.2480e-02, -1.7090e-01, -5.9570e-02, -6.1523e-02,  4.7974e-02,\n",
       "         4.3488e-02,  2.0752e-01,  3.9258e-01, -6.2500e-02, -1.6357e-02,\n",
       "        -1.5051e-01,  1.6406e-01, -1.6748e-01, -1.5381e-01, -8.1177e-02,\n",
       "        -2.0264e-02,  4.1016e-02, -4.8828e-04,  2.4902e-02,  5.2368e-02,\n",
       "        -5.0781e-02, -1.8872e-01, -1.5503e-01, -9.6191e-02,  7.1289e-02,\n",
       "         2.1313e-01,  2.5146e-02, -1.2445e-01,  1.6602e-01, -2.6855e-02,\n",
       "         1.3611e-02, -1.5625e-02, -2.0508e-02, -3.3569e-02, -1.6809e-01,\n",
       "        -1.9287e-01, -1.1304e-01,  2.5574e-02, -2.6489e-01,  1.0425e-01,\n",
       "         7.2510e-02, -2.3926e-02, -1.3574e-01,  1.7334e-02,  3.5889e-02,\n",
       "         1.6064e-01,  4.0039e-02, -2.2461e-02, -5.2734e-02, -1.3770e-01,\n",
       "         7.2510e-02,  3.4180e-02,  8.5571e-02, -3.4912e-02,  5.7495e-02,\n",
       "        -4.8584e-02,  2.2119e-01, -1.1279e-01,  1.5381e-02, -1.5283e-01,\n",
       "        -7.9834e-02,  0.0000e+00, -8.4961e-02, -2.5391e-02,  5.0293e-02,\n",
       "        -5.8838e-02, -1.0596e-01,  9.4238e-02, -7.8125e-02,  4.1870e-02,\n",
       "        -1.3428e-03,  8.1055e-02,  1.8677e-01, -3.0273e-02, -1.7090e-01,\n",
       "        -1.3428e-01,  6.7139e-02, -3.1250e-01, -3.5645e-01, -9.5459e-02,\n",
       "        -9.0637e-02, -5.3711e-02, -6.1035e-03, -9.3384e-02,  1.0083e-01,\n",
       "         8.4167e-02,  2.3145e-01,  2.2998e-01,  2.8809e-02,  8.3496e-02,\n",
       "         6.4087e-02,  1.6211e-01, -7.8125e-02,  5.3955e-02,  2.3315e-02,\n",
       "        -5.3711e-03,  6.3599e-02, -2.6611e-02, -2.3975e-01,  3.9062e-03,\n",
       "        -5.6641e-02, -1.8091e-01, -3.9062e-02,  1.3452e-01,  5.2612e-02,\n",
       "        -8.4534e-02,  3.3496e-01,  6.2500e-02,  4.9194e-02,  9.1553e-03,\n",
       "         2.0508e-02,  1.1426e-01, -5.6366e-02, -1.8555e-02,  1.1499e-01,\n",
       "         3.7781e-02,  6.5430e-02,  2.6318e-01,  3.0273e-02,  6.0303e-02,\n",
       "        -1.6138e-01,  1.6589e-01, -1.7773e-01,  5.7373e-02,  1.2598e-01,\n",
       "         5.7983e-02,  2.4414e-04, -1.3403e-01,  1.4600e-01,  3.1494e-02,\n",
       "        -1.2598e-01, -2.7344e-02,  7.5195e-02,  7.5684e-02, -5.0781e-02,\n",
       "         2.0996e-02, -1.2866e-01, -1.5283e-01, -1.1816e-01,  1.3098e-01,\n",
       "         6.7871e-02,  1.8555e-01,  1.7261e-01,  1.6553e-01,  2.0020e-02,\n",
       "        -1.6956e-01, -7.4219e-02, -8.1055e-02, -2.5391e-02,  6.3477e-02,\n",
       "        -9.3262e-02, -2.5909e-02,  1.6113e-02,  1.1426e-01, -1.1328e-01,\n",
       "         6.5186e-02, -1.0303e-01,  1.2939e-02, -2.1362e-01,  1.2378e-01,\n",
       "        -1.2402e-01, -1.4648e-02, -6.3232e-02, -9.6924e-02, -1.0059e-01,\n",
       "         2.9297e-03,  5.6396e-02, -3.1555e-02,  1.1169e-01, -8.7891e-03,\n",
       "         4.5166e-02,  5.0293e-02,  9.0820e-02,  1.2988e-01, -3.4668e-02,\n",
       "         4.8462e-02,  1.8066e-02, -1.6602e-02,  2.9541e-02,  4.1260e-02,\n",
       "         1.6821e-01,  1.1877e-01,  5.2429e-02, -3.4155e-01, -3.7158e-01,\n",
       "        -7.3975e-02, -1.9006e-01, -1.9043e-01,  4.9805e-02,  4.2236e-02,\n",
       "        -1.4893e-01,  1.2646e-01,  7.2266e-02,  1.1230e-02, -7.6660e-02,\n",
       "        -3.7109e-02,  1.1816e-01, -3.4912e-02,  2.6855e-02,  1.8213e-01,\n",
       "        -1.2720e-01, -4.4922e-02,  7.5195e-02,  5.7373e-02, -2.0312e-01,\n",
       "         1.0559e-01, -5.5176e-02, -7.6172e-02, -1.6357e-02, -2.0129e-01,\n",
       "         1.2500e-01,  9.9609e-02,  5.6152e-03, -1.1426e-01, -2.5537e-01,\n",
       "        -5.3345e-02,  2.5391e-02, -1.9043e-02,  1.9409e-01, -7.8369e-02,\n",
       "        -3.4912e-02,  8.5938e-02,  9.2651e-02,  7.7637e-02, -1.1987e-01,\n",
       "         9.4238e-02, -8.6426e-02, -5.3467e-02, -1.6016e-01,  1.8359e-01,\n",
       "         9.2163e-02, -6.2988e-02,  4.1748e-02, -6.3477e-03,  1.6113e-02,\n",
       "         6.9336e-02, -2.6318e-01,  2.0508e-02, -1.7285e-01,  4.5410e-02,\n",
       "        -1.1707e-01, -1.1719e-01,  2.0264e-02, -1.7212e-02, -6.3965e-02,\n",
       "         8.3252e-02, -1.5918e-01,  1.9775e-02,  2.0898e-01,  6.9092e-02,\n",
       "         4.8828e-02, -1.6211e-01, -7.0435e-02,  6.9214e-02,  1.0425e-01,\n",
       "         3.2593e-02, -8.1055e-02,  9.4604e-02, -8.6792e-02,  6.4941e-02,\n",
       "        -1.4844e-01,  1.6357e-02, -1.3159e-01,  9.2285e-02, -8.3435e-02,\n",
       "        -2.9541e-02,  6.2866e-02, -1.7432e-01,  1.0547e-01,  9.3994e-02,\n",
       "        -8.8867e-02, -4.1016e-02, -5.8594e-03,  5.9570e-02, -7.0923e-02,\n",
       "        -3.9062e-03, -2.2510e-01,  1.6968e-02, -9.0332e-02,  7.7148e-02,\n",
       "         3.9062e-03, -1.9360e-01, -6.4697e-03,  2.7588e-02,  7.4219e-02,\n",
       "        -9.7656e-02, -7.6172e-02,  2.2510e-01,  1.9434e-01, -4.8340e-02,\n",
       "         1.1963e-01,  1.0376e-03, -2.3438e-02, -5.2734e-02,  3.4302e-02,\n",
       "         1.1377e-01, -9.8389e-02,  2.1973e-02,  1.2695e-02,  5.1270e-02,\n",
       "         1.4697e-01,  5.7739e-02, -1.5308e-01, -1.6846e-01, -3.2227e-02,\n",
       "        -1.1719e-01,  4.0771e-02,  6.1279e-02,  6.1340e-02,  8.2520e-02,\n",
       "        -4.8828e-04,  3.4973e-02, -7.8613e-02, -2.2766e-02,  9.0149e-02,\n",
       "        -1.2012e-01,  5.4688e-02,  1.9409e-02, -4.5288e-02, -4.2969e-02,\n",
       "         1.3770e-01,  3.1738e-02,  2.2778e-01,  3.9062e-02,  1.2305e-01,\n",
       "        -2.4414e-04,  2.4670e-01], dtype=torch.float16)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = r[3, 0]\n",
    "r2 = r[3, 3]\n",
    "\n",
    "print(r1.size(), r2.size())\n",
    "((r1 - r2) == 0).sum()\n",
    "r1-r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-7.3254e-05, -2.4414e-04,  4.8828e-04, -2.4438e-05, -6.1035e-05,\n",
       "         0.0000e+00, -2.4438e-05,  1.0985e-04, -9.7632e-05,  9.7632e-05,\n",
       "         2.4438e-05, -1.5259e-05, -3.9053e-04,  4.2737e-05,  2.4414e-04,\n",
       "        -3.4189e-04,  0.0000e+00,  2.6846e-04,  1.5259e-05, -1.2219e-05,\n",
       "        -1.8299e-05,  0.0000e+00,  3.6597e-05, -6.8378e-04, -1.4651e-04,\n",
       "         0.0000e+00,  0.0000e+00,  4.8816e-05,  1.4651e-04,  1.3423e-04,\n",
       "        -4.8816e-05,  7.3254e-05, -2.4438e-05,  2.4438e-05, -1.2207e-04,\n",
       "         2.4438e-05,  0.0000e+00,  1.2207e-04,  0.0000e+00,  1.2207e-04,\n",
       "        -2.4438e-05,  1.4651e-04, -1.5497e-06, -6.1035e-05, -1.2219e-05,\n",
       "        -4.8816e-05, -7.3254e-05, -1.7095e-04, -3.6597e-05,  9.7632e-05,\n",
       "         1.2207e-04,  4.2737e-05,  1.8299e-05, -7.3254e-05,  0.0000e+00,\n",
       "        -3.6597e-05,  2.9302e-04, -3.4189e-04,  4.8816e-05,  1.4651e-04,\n",
       "        -9.7632e-05,  1.4651e-04,  6.1035e-05, -4.8816e-05,  0.0000e+00,\n",
       "         2.4438e-05,  1.2207e-04,  2.4438e-05, -6.1035e-05, -2.4438e-05,\n",
       "        -1.7095e-04,  2.4438e-05, -4.8828e-04,  9.7632e-05, -2.4438e-05,\n",
       "         4.8816e-05,  0.0000e+00, -1.4651e-04,  4.8816e-05, -1.2219e-05,\n",
       "         4.8816e-05,  4.8816e-05, -3.9053e-04,  1.2207e-04, -1.5497e-06,\n",
       "         2.4414e-04,  4.8816e-05,  2.9302e-04,  9.7632e-05, -3.9053e-04,\n",
       "        -1.9526e-04, -4.2737e-05, -3.9053e-04, -6.8545e-06,  4.8816e-05,\n",
       "         4.2737e-05, -4.8816e-05, -8.5473e-05,  9.7632e-05,  2.4438e-05,\n",
       "        -1.7095e-04,  2.7478e-05,  6.0797e-06,  1.7095e-04, -1.8299e-05,\n",
       "         6.0797e-06,  0.0000e+00, -5.4955e-05,  7.3254e-05,  1.4651e-04,\n",
       "        -2.4438e-05, -1.8299e-05,  2.4438e-05,  6.0797e-06,  0.0000e+00,\n",
       "        -1.2207e-04, -6.0797e-06,  2.4438e-05,  0.0000e+00, -1.5259e-05,\n",
       "         2.6846e-04,  9.7632e-05,  1.2219e-05,  1.2219e-05, -1.2219e-05,\n",
       "         1.2207e-04,  0.0000e+00, -9.7632e-05,  1.7095e-04,  1.5497e-06,\n",
       "        -9.7632e-05, -1.4651e-04,  4.8816e-05, -9.7632e-05,  0.0000e+00,\n",
       "         6.1035e-05,  9.7632e-05,  0.0000e+00, -1.4651e-04,  2.4438e-05,\n",
       "         0.0000e+00,  0.0000e+00, -2.4438e-05, -6.0797e-06,  1.3423e-04,\n",
       "         2.4438e-05, -1.7095e-04,  1.2219e-05, -7.3254e-05, -2.4438e-05,\n",
       "        -1.9526e-04, -2.4438e-05,  1.7095e-04, -1.2219e-05, -4.8816e-05,\n",
       "         2.4438e-05,  3.9053e-04, -4.8816e-05, -3.0518e-05, -1.4651e-04,\n",
       "        -4.8816e-05,  2.4438e-05,  0.0000e+00, -9.1791e-06, -3.0518e-05,\n",
       "         1.7095e-04,  2.4438e-05,  1.9526e-04,  0.0000e+00, -8.5473e-05,\n",
       "        -8.5473e-05,  7.3254e-05, -4.8816e-05,  3.9053e-04, -1.2207e-04,\n",
       "         4.8816e-05, -4.8816e-05,  7.9870e-06, -7.3254e-05,  0.0000e+00,\n",
       "         4.8816e-05,  9.1791e-06,  2.9302e-04, -9.7632e-05,  2.4414e-04,\n",
       "        -1.9526e-04,  1.9526e-04,  0.0000e+00,  3.9053e-04,  6.1035e-05,\n",
       "        -1.5497e-06, -9.7632e-05, -3.6597e-05,  0.0000e+00, -1.2219e-05,\n",
       "         1.2219e-05,  9.7632e-05,  3.6597e-05,  2.4438e-05,  3.0518e-05,\n",
       "        -2.4438e-05,  0.0000e+00, -7.3254e-05,  8.5473e-05, -3.6597e-05,\n",
       "        -9.1791e-06,  0.0000e+00, -3.6597e-05,  1.3423e-04, -2.4414e-04,\n",
       "        -1.2219e-05, -9.7632e-05, -1.5259e-05,  3.6597e-05, -1.7095e-04,\n",
       "        -1.2219e-05, -1.3423e-04,  9.7632e-05,  1.2219e-05,  9.1791e-06,\n",
       "        -1.9526e-04,  2.4438e-05,  3.0518e-05, -6.0797e-06, -2.4438e-05,\n",
       "        -8.5473e-05, -2.4438e-05, -2.4438e-05,  4.8816e-05,  2.4438e-05,\n",
       "         1.9526e-04, -1.4651e-04,  1.9526e-04, -4.8816e-05,  3.6621e-04,\n",
       "        -1.0985e-04,  2.1338e-05,  7.6294e-06,  9.7632e-05,  1.3423e-04,\n",
       "        -9.7632e-05, -2.4438e-05,  4.8816e-05,  9.7632e-05, -7.3254e-05,\n",
       "        -1.2219e-05, -1.2207e-04,  2.9302e-04,  1.4651e-04,  0.0000e+00,\n",
       "         0.0000e+00,  9.7632e-05,  6.0797e-06, -6.0797e-06,  1.5867e-04,\n",
       "        -6.0797e-06, -4.8816e-05, -2.4438e-05, -4.8816e-05,  1.4651e-04,\n",
       "        -4.8816e-05,  7.3254e-05, -2.9302e-04, -2.2113e-05,  2.4438e-05,\n",
       "         7.3254e-05,  1.9526e-04,  1.2219e-05,  6.1035e-05,  1.5259e-05,\n",
       "         1.8299e-05,  0.0000e+00,  2.4438e-05,  1.8299e-05,  3.4189e-04,\n",
       "         6.0797e-06,  9.7632e-05, -1.4651e-04, -4.8816e-05,  9.7632e-05,\n",
       "        -1.2219e-05, -2.4438e-05, -1.2207e-04,  7.3254e-05,  2.1970e-04,\n",
       "        -6.0797e-06,  4.8816e-05, -9.7632e-05,  1.2207e-04,  0.0000e+00,\n",
       "         6.0797e-06, -1.4651e-04,  1.8299e-05, -1.2219e-05,  1.2219e-05,\n",
       "         1.2219e-05,  1.0669e-05, -2.1338e-05, -9.7632e-05,  6.1035e-05,\n",
       "         1.8299e-05, -1.9526e-04,  9.7632e-05, -2.4414e-04,  0.0000e+00,\n",
       "         7.3254e-05,  3.6597e-05, -4.8816e-05, -6.0797e-06,  1.9526e-04,\n",
       "        -6.1035e-05, -4.8816e-05,  9.7632e-05, -1.0985e-04,  9.7632e-05,\n",
       "        -3.9053e-04,  3.6597e-05,  1.2219e-05,  1.7095e-04,  2.1970e-04,\n",
       "        -4.8816e-05, -1.2219e-05,  4.8816e-05, -9.7632e-05, -7.3254e-05,\n",
       "         3.6597e-05, -2.4414e-04, -1.2219e-05, -2.4438e-05, -6.0797e-06,\n",
       "         1.2219e-05,  2.4414e-04,  1.4651e-04,  0.0000e+00, -4.8816e-05,\n",
       "        -1.4651e-04,  1.2219e-05, -3.4189e-04,  9.7632e-05,  2.4414e-04,\n",
       "         7.3254e-05,  1.4651e-04,  2.4438e-05, -1.0669e-05,  2.4438e-05,\n",
       "         2.6846e-04,  1.4651e-04, -1.2219e-05,  0.0000e+00, -7.3254e-05,\n",
       "        -1.4651e-04,  0.0000e+00, -1.2219e-05, -1.0985e-04,  1.4651e-04,\n",
       "         3.6597e-05, -3.6597e-05, -1.9526e-04,  2.4438e-05, -9.7632e-05,\n",
       "        -2.4438e-05,  2.4438e-05, -4.8816e-05, -4.5896e-06,  1.2219e-05,\n",
       "         0.0000e+00,  0.0000e+00, -6.1035e-05, -6.1035e-05,  1.9526e-04,\n",
       "         0.0000e+00, -1.2219e-05,  1.9526e-04, -6.0797e-06, -6.1035e-05,\n",
       "        -4.8816e-05, -3.9053e-04,  9.7632e-05, -1.8299e-05, -4.8816e-05,\n",
       "         4.8816e-05, -4.8816e-05,  6.0797e-06, -3.4189e-04,  0.0000e+00,\n",
       "         9.7632e-05, -7.3254e-05, -4.8816e-05, -1.8299e-05, -2.4438e-05,\n",
       "         1.2815e-04, -3.6597e-05,  0.0000e+00, -4.8816e-05,  1.3423e-04,\n",
       "         2.4414e-04,  2.6846e-04, -4.8816e-05, -2.1970e-04,  3.6597e-05,\n",
       "         0.0000e+00, -2.9302e-04, -8.5473e-05,  3.6597e-05, -6.1035e-05,\n",
       "         1.4651e-04, -8.5473e-05,  0.0000e+00,  2.4438e-05,  1.4651e-04,\n",
       "         2.1970e-04, -9.7632e-05, -4.8816e-05,  4.8828e-04, -6.0797e-06,\n",
       "         4.8816e-05,  1.4651e-04,  0.0000e+00, -1.9526e-04,  1.2207e-04,\n",
       "        -6.1035e-05,  1.9526e-04,  4.3941e-04, -3.6597e-05,  0.0000e+00,\n",
       "        -3.0398e-06,  8.5473e-05, -4.8816e-05, -1.5259e-05, -1.4651e-04,\n",
       "        -1.4651e-04,  9.7632e-05,  1.2219e-05, -3.9053e-04, -9.7632e-05,\n",
       "         1.9526e-04, -9.7632e-05,  6.1035e-05, -3.6597e-05,  4.8816e-05,\n",
       "        -1.8299e-05,  2.9302e-04, -6.7115e-05, -8.5473e-05, -1.9526e-04,\n",
       "         1.5259e-05,  0.0000e+00,  4.8816e-05,  6.0797e-06,  1.8299e-05,\n",
       "        -1.2219e-05,  6.7115e-05,  2.4414e-04, -1.7095e-04,  4.8816e-05,\n",
       "         4.8816e-05, -3.4189e-04, -2.4438e-05, -1.2207e-04,  0.0000e+00,\n",
       "         3.4189e-04, -4.8816e-05,  7.9334e-05, -1.8299e-05,  1.9526e-04,\n",
       "        -2.4414e-04,  2.4438e-05,  2.4438e-05,  0.0000e+00,  1.5867e-04,\n",
       "         4.8816e-05,  6.0797e-06,  0.0000e+00,  4.2737e-05, -2.4438e-05,\n",
       "         8.5473e-05,  0.0000e+00,  0.0000e+00, -2.4414e-04, -1.2219e-05,\n",
       "         0.0000e+00, -1.2219e-05, -2.4438e-05, -6.0797e-06,  9.7632e-05,\n",
       "         7.3254e-05,  1.0985e-04,  1.0669e-05, -1.3423e-04, -4.8828e-04,\n",
       "        -1.2219e-05, -2.4438e-05, -4.8816e-05,  6.0797e-06,  0.0000e+00,\n",
       "         1.2207e-04, -4.2737e-05,  4.8816e-05, -8.5473e-05,  0.0000e+00,\n",
       "         2.2650e-06, -1.2219e-05, -3.0398e-06,  3.6597e-05,  1.9526e-04,\n",
       "        -3.9053e-04, -4.3941e-04, -4.8816e-05,  4.3941e-04,  4.8816e-05,\n",
       "         1.2207e-04,  1.8299e-05], dtype=torch.float16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = r[3]\n",
    "(r1 - r1.mean(dim=0, keepdim=True)).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [00:00<00:00, 94797.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 1\n",
      "0 2\n",
      "0 3\n",
      "0 4\n",
      "0 5\n",
      "0 6\n",
      "1 0\n",
      "1 1\n",
      "1 2\n",
      "1 3\n",
      "1 4\n",
      "1 5\n",
      "1 6\n",
      "2 0\n",
      "2 1\n",
      "2 2\n",
      "2 3\n",
      "2 4\n",
      "2 5\n",
      "2 6\n",
      "3 0\n",
      "3 1\n",
      "3 2\n",
      "3 3\n",
      "3 4\n",
      "3 5\n",
      "3 6\n",
      "4 0\n",
      "4 1\n",
      "4 2\n",
      "4 3\n",
      "4 4\n",
      "4 5\n",
      "4 6\n",
      "5 0\n",
      "5 1\n",
      "5 2\n",
      "5 3\n",
      "5 4\n",
      "5 5\n",
      "5 6\n",
      "6 0\n",
      "6 1\n",
      "6 2\n",
      "6 3\n",
      "6 4\n",
      "6 5\n",
      "6 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_layers1 = r.size(0)\n",
    "n_layers2 = r.size(0)\n",
    "for rep1_layer_idx, rep2_layer_idx in tqdm(\n",
    "    itertools.product(range(n_layers1), range(n_layers2)), total=n_layers1 * n_layers2\n",
    "):\n",
    "    print(rep1_layer_idx, rep2_layer_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ValueError: array must not contain infs or NaNs` when trying to compare layer zero to six in main code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infs tensor(0)\n",
      "nans tensor(0)\n"
     ]
    }
   ],
   "source": [
    "def check_nans_and_infs(r):\n",
    "    if isinstance(r, torch.Tensor):\n",
    "        print(\"infs\", r.isinf().sum())\n",
    "        print(\"nans\", r.isnan().sum())\n",
    "    elif isinstance(r, np.ndarray):\n",
    "        print(\"infs\", np.isinf(r).sum())\n",
    "        print(\"nans\", np.isnan(r).sum())\n",
    "    else:\n",
    "        raise ValueError(f\"{type(r)}\")\n",
    "check_nans_and_infs(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anfangs keine infs or nans --> kommt vom preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "centering\n",
      "zeros 0\n",
      "zeros 648704\n",
      "zeros share 1.0\n",
      "infs 0\n",
      "nans 0\n",
      "1\n",
      "centering\n",
      "zeros 13\n",
      "zeros 1678\n",
      "zeros share 0.0025866959352801896\n",
      "infs 0\n",
      "nans 0\n",
      "2\n",
      "centering\n",
      "zeros 89\n",
      "zeros 1355\n",
      "zeros share 0.002088780090765588\n",
      "infs 0\n",
      "nans 0\n",
      "3\n",
      "centering\n",
      "zeros 68\n",
      "zeros 820\n",
      "zeros share 0.0012640588003157065\n",
      "infs 0\n",
      "nans 0\n",
      "4\n",
      "centering\n",
      "zeros 183\n",
      "zeros 712\n",
      "zeros share 0.0010975730071033937\n",
      "infs 0\n",
      "nans 0\n",
      "5\n",
      "centering\n",
      "zeros 137\n",
      "zeros 359\n",
      "zeros share 0.0005534111089187056\n",
      "infs 0\n",
      "nans 0\n",
      "6\n",
      "centering\n",
      "zeros 0\n",
      "zeros 4369\n",
      "zeros share 0.006734966949486977\n",
      "infs 0\n",
      "nans 0\n"
     ]
    }
   ],
   "source": [
    "def to_numpy_if_needed(*args: Union[torch.Tensor, npt.NDArray]) -> List[npt.NDArray]:\n",
    "    def convert(x: Union[torch.Tensor, npt.NDArray]) -> npt.NDArray:\n",
    "        return x if isinstance(x, np.ndarray) else x.numpy()\n",
    "\n",
    "    return list(map(convert, args))\n",
    "\n",
    "def center_columns(R: npt.NDArray) -> npt.NDArray:\n",
    "    return R - R.mean(axis=0)[None, :]\n",
    "\n",
    "\n",
    "def normalize_matrix_norm(R: npt.NDArray) -> npt.NDArray:\n",
    "    return R / np.linalg.norm(R, ord=\"fro\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for layer in range(r.size(0)):\n",
    "    # conversion seems fine\n",
    "    # print(\"numpy conversion\")\n",
    "    # check_nans_and_infs(r_np)\n",
    "    r_np = to_numpy_if_needed(r)[0]\n",
    "    print(layer)\n",
    "    print(\"centering\")\n",
    "    r_np = r_np[layer]\n",
    "    print(\"zeros\", (r_np == 0).sum())\n",
    "    r_np_nonzeroed = r_np\n",
    "    r_np = center_columns(r_np)\n",
    "    print(\"zeros\", (r_np == 0).sum())\n",
    "    print(\"zeros share\", (r_np == 0).sum()/r_np.size)\n",
    "    check_nans_and_infs(r_np)\n",
    "\n",
    "# normalize fails, because all vals are zero\n",
    "# print(\"normalize\")\n",
    "# print(r_np.shape)\n",
    "# print((r_np == 0).sum())\n",
    "# r_np = normalize_matrix_norm(r_np)\n",
    "# check_nans_and_infs(r_np)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layer zero normalization probably fails, because it is the embedding output and all prompts end the same way. \n",
    "Hence, the final token is always identical, and centering thus turns the columns into zeros.\n",
    "\n",
    "_Maybe not true: in the main script it fails with layer 6..._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01238 , -0.01467 ,  0.014946, ..., -0.02074 , -0.0531  ,\n",
       "        -0.01828 ],\n",
       "       [-0.01238 , -0.01467 ,  0.014946, ..., -0.02074 , -0.0531  ,\n",
       "        -0.01828 ],\n",
       "       [-0.01238 , -0.01467 ,  0.014946, ..., -0.02074 , -0.0531  ,\n",
       "        -0.01828 ],\n",
       "       ...,\n",
       "       [-0.01238 , -0.01467 ,  0.014946, ..., -0.02074 , -0.0531  ,\n",
       "        -0.01828 ],\n",
       "       [-0.01238 , -0.01467 ,  0.014946, ..., -0.02074 , -0.0531  ,\n",
       "        -0.01828 ],\n",
       "       [-0.01238 , -0.01467 ,  0.014946, ..., -0.02074 , -0.0531  ,\n",
       "        -0.01828 ]], dtype=float16)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_np_nonzeroed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
