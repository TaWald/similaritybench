{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d1ec974ce8f4128a8f7949b1af7b8ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/68 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "712b27524c5248d9bafa2b17a9e88c31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71cedb9b765e4730ab837810c73a9862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "from pathlib import Path\n",
    "from repsim.utils import convert_to_path_compatible\n",
    "\n",
    "path = \"sst2\"\n",
    "output_dir = Path(\"../experiments/datasets/nlp/robustness/\") / convert_to_path_compatible(path)\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "split = \"validation\"\n",
    "\n",
    "ds = datasets.load_dataset(path)\n",
    "for name, subset in ds.items():\n",
    "    subset.to_csv(output_dir / f\"{name}.csv\", columns=[\"sentence\", \"label\"], sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance of SST2-finetuned model on standard SST2 validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-01 11:01:38.210890: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-01 11:01:38.892753: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9162844036697247, 'total_time_in_seconds': 21.625845240429044, 'samples_per_second': 40.32212338086167, 'latency_in_seconds': 0.024800281239024134}\n"
     ]
    }
   ],
   "source": [
    "from evaluate import evaluator\n",
    "import evaluate\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"/root/LLM-comparison/outputs/2024-01-31/13-12-49\",\n",
    "    tokenizer=\"google/multiberts-seed_0\",\n",
    "    device=0,\n",
    "    max_length=128\n",
    ")\n",
    "data = ds[split]  # .shuffle().select(range(1000))\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "task_evaluator = evaluator(\"text-classification\")\n",
    "\n",
    "results = task_evaluator.compute(\n",
    "    model_or_pipeline=pipe,\n",
    "    data=data,\n",
    "    metric=metric,\n",
    "    label_mapping={\"LABEL_0\": 0, \"LABEL_1\": 1},\n",
    "    input_column=\"sentence\"\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create augmented validation set of SST2.\n",
    "\n",
    "`--transformations-per-example 4` means that there will be 4 augmented sentences per standard sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-01 11:31:06.818233: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[34;1mtextattack\u001b[0m: Preparing to overwrite ../experiments/datasets/nlp/robustness/sst2/validation_augmented.csv.\n",
      "\u001b[34;1mtextattack\u001b[0m: Read 872 rows from ../experiments/datasets/nlp/robustness/sst2/validation.csv. Found columns {'sentence', 'label'}.\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Augmenting rows: 100%|████████████████████████| 872/872 [02:59<00:00,  4.85it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Wrote 872 augmentations to ../experiments/datasets/nlp/robustness/sst2/validation_augmented.csv in 181.3689239025116s.\n"
     ]
    }
   ],
   "source": [
    "!textattack augment --input-csv {output_dir / \"validation.csv\"}\\\n",
    "    --output-csv {output_dir / \"validation_augmented.csv\"}\\\n",
    "    --input-column sentence\\\n",
    "    --recipe eda\\\n",
    "    --pct-words-to-swap  0.8\\\n",
    "    --transformations-per-example 1\\\n",
    "    --random-seed 123\\\n",
    "    --exclude-original\\\n",
    "    --overwrite\\\n",
    "    --fast_augment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance of model trained on standard data on augmented data.\n",
    "\n",
    "\n",
    "| Accuracy | Swapped | N_Transformations | Seed |\n",
    "|----------|---------|-------------------|------|\n",
    "|  0.9162  |    -    |      -            |  -   |\n",
    "|  0.9037  |   0.1   |             1     | 123  |\n",
    "|  0.8154  |   0.5   |             1     | 123  |\n",
    "|  0.7569  |   0.8   |             1     | 123  |\n",
    "\n",
    "0.817 acc für finetuned model auf augmented data nach 10 epochs mit 0.8 swapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e2cbe12197e461fa40e48ca357a3dac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/similaritybench/.venv/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:778: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on standard data: 0.9162844036697247\n",
      "Accuracy on augmented data: 0.7568807339449541\n"
     ]
    }
   ],
   "source": [
    "data = datasets.load_dataset(\"csv\", data_files=str(output_dir / \"validation_augmented.csv\"))[\"train\"]\n",
    "\n",
    "results_augmented = task_evaluator.compute(\n",
    "    model_or_pipeline=pipe,\n",
    "    data=data,\n",
    "    metric=metric,\n",
    "    label_mapping={\"LABEL_0\": 0, \"LABEL_1\": 1},\n",
    "    input_column=\"sentence\"\n",
    ")\n",
    "\n",
    "print(f\"Accuracy on standard data: {results['accuracy']}\")\n",
    "print(f\"Accuracy on augmented data: {results_augmented['accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do actual augmentation for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_words_to_swap = str(0.8)\n",
    "transformations_per_example = str(1)\n",
    "seed = str(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-01 11:49:34.937445: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[34;1mtextattack\u001b[0m: Read 67349 rows from ../experiments/datasets/nlp/robustness/sst2/train.csv. Found columns {'label', 'sentence'}.\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Augmenting rows: 100%|██████████████████| 67349/67349 [1:11:29<00:00, 15.70it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Wrote 67349 augmentations to ../experiments/datasets/nlp/robustness/sst2/train_augmented.csv in 4292.183369398117s.\n",
      "2024-03-01 13:01:16.521153: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[34;1mtextattack\u001b[0m: Preparing to overwrite ../experiments/datasets/nlp/robustness/sst2/validation_augmented.csv.\n",
      "\u001b[34;1mtextattack\u001b[0m: Read 872 rows from ../experiments/datasets/nlp/robustness/sst2/validation.csv. Found columns {'label', 'sentence'}.\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Augmenting rows: 100%|████████████████████████| 872/872 [03:05<00:00,  4.70it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Wrote 872 augmentations to ../experiments/datasets/nlp/robustness/sst2/validation_augmented.csv in 187.38752579689026s.\n",
      "2024-03-01 13:04:32.346888: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[34;1mtextattack\u001b[0m: Read 1821 rows from ../experiments/datasets/nlp/robustness/sst2/test.csv. Found columns {'label', 'sentence'}.\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Augmenting rows: 100%|██████████████████████| 1821/1821 [06:02<00:00,  5.03it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Wrote 1821 augmentations to ../experiments/datasets/nlp/robustness/sst2/test_augmented.csv in 363.6330654621124s.\n"
     ]
    }
   ],
   "source": [
    "!textattack augment --input-csv {output_dir / \"train.csv\"}\\\n",
    "    --output-csv {output_dir / \"train_augmented.csv\"}\\\n",
    "    --input-column sentence\\\n",
    "    --recipe eda\\\n",
    "    --pct-words-to-swap  {pct_words_to_swap}\\\n",
    "    --transformations-per-example {transformations_per_example}\\\n",
    "    --random-seed {seed}\\\n",
    "    --exclude-original\\\n",
    "    --overwrite\\\n",
    "    --fast_augment\n",
    "\n",
    "!textattack augment --input-csv {output_dir / \"validation.csv\"}\\\n",
    "    --output-csv {output_dir / \"validation_augmented.csv\"}\\\n",
    "    --input-column sentence\\\n",
    "    --recipe eda\\\n",
    "    --pct-words-to-swap  {pct_words_to_swap}\\\n",
    "    --transformations-per-example {transformations_per_example}\\\n",
    "    --random-seed {seed}\\\n",
    "    --exclude-original\\\n",
    "    --overwrite\\\n",
    "    --fast_augment\n",
    "\n",
    "!textattack augment --input-csv {output_dir / \"test.csv\"}\\\n",
    "    --output-csv {output_dir / \"test_augmented.csv\"}\\\n",
    "    --input-column sentence\\\n",
    "    --recipe eda\\\n",
    "    --pct-words-to-swap  {pct_words_to_swap}\\\n",
    "    --transformations-per-example {transformations_per_example}\\\n",
    "    --random-seed {seed}\\\n",
    "    --exclude-original\\\n",
    "    --overwrite\\\n",
    "    --fast_augment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the performance of the model trained on augmented data on normal data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9013761467889908, 'total_time_in_seconds': 32.900379156693816, 'samples_per_second': 26.50425382172489, 'latency_in_seconds': 0.0377297926108874}\n"
     ]
    }
   ],
   "source": [
    "from evaluate import evaluator\n",
    "import evaluate\n",
    "import datasets\n",
    "from transformers import pipeline\n",
    "\n",
    "path = \"sst2\"\n",
    "split = \"validation\"\n",
    "ds = datasets.load_dataset(path)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-classification\",\n",
    "    # model=\"/root/similaritybench/outputs/2024-03-01/13-45-35\",\n",
    "    model=\"/root/similaritybench/outputs/2024-03-04/11-11-12/checkpoint-31000\",\n",
    "    tokenizer=\"google/multiberts-seed_0\",\n",
    "    device=0,\n",
    "    max_length=128\n",
    ")\n",
    "data = ds[split]  # .shuffle().select(range(1000))\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "task_evaluator = evaluator(\"text-classification\")\n",
    "\n",
    "results = task_evaluator.compute(\n",
    "    model_or_pipeline=pipe,\n",
    "    data=data,\n",
    "    metric=metric,\n",
    "    label_mapping={\"LABEL_0\": 0, \"LABEL_1\": 1},\n",
    "    input_column=\"sentence\"\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
