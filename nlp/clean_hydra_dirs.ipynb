{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "from omegaconf import OmegaConf\n",
    "from typing import Callable\n",
    "import warnings\n",
    "\n",
    "\n",
    "def datasets_to_storage(\n",
    "    path: Path, is_date_dir: bool, is_multirun_dir: bool, target_dir: Path, subdir_id_creator: Callable\n",
    "):\n",
    "    if not is_date_dir or not is_multirun_dir:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    for timestamp_dir in path.iterdir():\n",
    "        if not timestamp_dir.is_dir():\n",
    "            continue\n",
    "\n",
    "        for run_dir in timestamp_dir.iterdir():\n",
    "            if not run_dir.is_dir():\n",
    "                continue\n",
    "\n",
    "            cfg = OmegaConf.load(run_dir / \".hydra\" / \"config.yaml\")\n",
    "            target_subdir = target_dir / subdir_id_creator(cfg)\n",
    "            target_subdir.mkdir(exist_ok=True)\n",
    "\n",
    "            shutil.copy2(str(run_dir / \"dataset_dict.json\"), target_subdir)\n",
    "\n",
    "            dirs = [d for d in run_dir.iterdir() if d.is_dir() and d.name != \"runs\"]  # filtering tensorflow logs\n",
    "            for d in dirs:\n",
    "                split_dir = target_subdir / d.name\n",
    "                if split_dir.exists():\n",
    "                    warnings.warn(f\"{split_dir} already exists. Skipping.\")\n",
    "                    continue\n",
    "                print(split_dir)\n",
    "                shutil.copytree(str(d), str(split_dir))\n",
    "\n",
    "\n",
    "def experiment_results_to_storage(\n",
    "    path: Path, is_date_dir: bool, is_multirun_dir: bool, target_dir: Path, subdir_id_creator: Callable\n",
    "):\n",
    "    if not is_date_dir or not is_multirun_dir:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    for timestamp_dir in path.iterdir():\n",
    "        if not timestamp_dir.is_dir():\n",
    "            continue\n",
    "\n",
    "        for run_dir in timestamp_dir.iterdir():\n",
    "            if not run_dir.is_dir():\n",
    "                continue\n",
    "\n",
    "            cfg = OmegaConf.load(run_dir / \".hydra\" / \"config.yaml\")\n",
    "            target_subdir = target_dir / subdir_id_creator(cfg)\n",
    "            target_subdir.mkdir(exist_ok=True)\n",
    "\n",
    "            weights = run_dir / \"model.safetensors\"\n",
    "            training_args = run_dir / \"training_args.bin\"\n",
    "            model_cfg = run_dir / \"config.json\"\n",
    "            hydra_cfg = run_dir / \".hydra\" / \"config.yaml\"\n",
    "            for f_to_be_copied in [weights, training_args, model_cfg, hydra_cfg]:\n",
    "                print(target_subdir / f_to_be_copied.name)\n",
    "                shutil.copy2(str(f_to_be_copied), str(target_subdir / f_to_be_copied.name))\n",
    "\n",
    "\n",
    "def delete_unused_checkpoints(path: Path, is_date_dir: bool, is_multirun_dir: bool):\n",
    "    if not is_date_dir or not is_multirun_dir:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    for timestamp_dir in path.iterdir():\n",
    "        if not timestamp_dir.is_dir():\n",
    "            continue\n",
    "\n",
    "        for run_dir in timestamp_dir.iterdir():\n",
    "            if not run_dir.is_dir():\n",
    "                continue\n",
    "\n",
    "            for dir_to_delete in run_dir.glob(\"checkpoint*\"):\n",
    "                print(f\"Deleting {dir_to_delete}\")\n",
    "                for obj in dir_to_delete.iterdir():\n",
    "                    obj.unlink()\n",
    "                dir_to_delete.rmdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortcut models\n",
    "\n",
    "- copy best model to `experiments` directory\n",
    "- delete unused checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"/root/similaritybench/multirun/2024-03-20\")\n",
    "is_date_dir = True\n",
    "is_multirun_dir = True\n",
    "target_dir = Path(\"/root/similaritybench/experiments/models/nlp/shortcut\")\n",
    "\n",
    "\n",
    "def shortcut_subdir_id(cfg):\n",
    "    dataset_id = cfg.dataset.path if not cfg.dataset.name else f\"{cfg.dataset.path}__{cfg.dataset.name}\"\n",
    "    shortcut_rate = str(cfg.shortcut_rate).replace(\".\", \"\")\n",
    "    return f\"{dataset_id}_pre{cfg.model.seed}_ft{cfg.dataset.finetuning.trainer.args.seed}_scrate{shortcut_rate}\"\n",
    "\n",
    "experiment_results_to_storage(path, is_date_dir, is_multirun_dir, target_dir, shortcut_subdir_id)\n",
    "delete_unused_checkpoints(path, is_date_dir, is_multirun_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New shortcut rates that take base rate of classes into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"/root/similaritybench/multirun/2024-04-02\")\n",
    "is_date_dir = True\n",
    "is_multirun_dir = True\n",
    "target_dir = Path(\"/root/similaritybench/experiments/models/nlp/shortcut\")\n",
    "\n",
    "\n",
    "def shortcut_subdir_id(cfg):\n",
    "    dataset_id = cfg.dataset.path if not cfg.dataset.name else f\"{cfg.dataset.path}__{cfg.dataset.name}\"\n",
    "    shortcut_rate = str(cfg.shortcut_rate).replace(\".\", \"\")\n",
    "    return f\"{dataset_id}_pre{cfg.model.seed}_ft{cfg.dataset.finetuning.trainer.args.seed}_scrate{shortcut_rate}\"\n",
    "\n",
    "experiment_results_to_storage(path, is_date_dir, is_multirun_dir, target_dir, shortcut_subdir_id)\n",
    "delete_unused_checkpoints(path, is_date_dir, is_multirun_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmented models\n",
    "\n",
    "(copied over from Viserion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"/root/multirun/2024-03-20\")\n",
    "is_date_dir = True\n",
    "is_multirun_dir = True\n",
    "target_dir = Path(\"/root/similaritybench/experiments/models/nlp/augmentation\")\n",
    "datasets_target_dir = Path(\"/root/similaritybench/experiments/datasets/nlp/robustness\")\n",
    "\n",
    "\n",
    "def augmentation_subdir_id(cfg):\n",
    "    dataset_id = cfg.dataset.path if not cfg.dataset.name else f\"{cfg.dataset.path}__{cfg.dataset.name}\"\n",
    "    augmentation_id = \"eda\"\n",
    "    strength = str(float(cfg.augmentation.recipe.pct_words_to_swap)).replace(\".\", \"\")\n",
    "    return f\"{dataset_id}_pre{cfg.model.seed}_ft{cfg.dataset.finetuning.trainer.args.seed}_{augmentation_id}_strength{strength}\"\n",
    "\n",
    "\n",
    "def augmentation_dataset_subdir_id(cfg):\n",
    "    dataset_id = cfg.dataset.path if not cfg.dataset.name else f\"{cfg.dataset.path}__{cfg.dataset.name}\"\n",
    "    augmentation_id = \"eda\"\n",
    "    strength = str(float(cfg.augmentation.recipe.pct_words_to_swap)).replace(\".\", \"\")\n",
    "    return f\"{dataset_id}_{augmentation_id}_strength{strength}\"\n",
    "\n",
    "\n",
    "# experiment_results_to_storage(path, is_date_dir, is_multirun_dir, target_dir, augmentation_subdir_id)\n",
    "# delete_unused_checkpoints(path, is_date_dir, is_multirun_dir)\n",
    "datasets_to_storage(path, is_date_dir, is_multirun_dir, datasets_target_dir, augmentation_dataset_subdir_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memorizing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_date_dir = True\n",
    "is_multirun_dir = True\n",
    "target_dir = Path(\"/root/similaritybench/experiments/models/nlp/memorizing\")\n",
    "datasets_target_dir = Path(\"/root/similaritybench/experiments/datasets/nlp/memorizing\")\n",
    "\n",
    "\n",
    "def memorizing_subdir_id(cfg):\n",
    "    dataset_id = cfg.dataset.path if not cfg.dataset.name else f\"{cfg.dataset.path}__{cfg.dataset.name}\"\n",
    "    strength = str(float(cfg.memorization_rate)).replace(\".\", \"\")\n",
    "    return f\"{dataset_id}_pre{cfg.model.seed}_ft{cfg.dataset.finetuning.trainer.args.seed}_labels{cfg.memorization_n_new_labels}_strength{strength}\"\n",
    "\n",
    "\n",
    "def memorizing_dataset_subdir_id(cfg):\n",
    "    dataset_id = cfg.dataset.path if not cfg.dataset.name else f\"{cfg.dataset.path}__{cfg.dataset.name}\"\n",
    "    strength = str(float(cfg.memorization_rate)).replace(\".\", \"\")\n",
    "    return f\"{dataset_id}_labels{cfg.memorization_n_new_labels}_strength{strength}\"\n",
    "\n",
    "\n",
    "path = Path(\"/root/similaritybench/multirun/2024-03-25\")\n",
    "experiment_results_to_storage(path, is_date_dir, is_multirun_dir, target_dir, memorizing_subdir_id)\n",
    "delete_unused_checkpoints(path, is_date_dir, is_multirun_dir)\n",
    "datasets_to_storage(path, is_date_dir, is_multirun_dir, datasets_target_dir, memorizing_dataset_subdir_id)\n",
    "\n",
    "path = Path(\"/root/similaritybench/multirun/2024-03-26\")\n",
    "experiment_results_to_storage(path, is_date_dir, is_multirun_dir, target_dir, memorizing_subdir_id)\n",
    "delete_unused_checkpoints(path, is_date_dir, is_multirun_dir)\n",
    "datasets_to_storage(path, is_date_dir, is_multirun_dir, datasets_target_dir, memorizing_dataset_subdir_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Models\n",
    "\n",
    "Models are already copied over, just need to add the hydra configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"/root/similaritybench/experiments/models/nlp/standard\")\n",
    "for model_path in path.iterdir():\n",
    "    with (model_path / \"path.txt\").open() as f:\n",
    "        original_hydra_path = f.readline()\n",
    "\n",
    "    config_path = Path(original_hydra_path) / \".hydra\" / \"config.yaml\"\n",
    "    print(config_path, model_path / config_path.name)\n",
    "    shutil.copy2(str(config_path), str(model_path / config_path.name))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
