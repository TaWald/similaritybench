{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating models on datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from evaluate import evaluator\n",
    "from omegaconf import OmegaConf\n",
    "from repsim.nlp import get_dataset, get_model\n",
    "from bert_finetune import ShortcutAdder\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "model_dirs = [\n",
    "    Path(\"/root/similaritybench/experiments/models/nlp/shortcut\"),\n",
    "]\n",
    "\n",
    "model_pattern = \"sst2*\"\n",
    "split = \"validation\"\n",
    "shortcut_rates = [0, 0.25, 0.5, 0.75, 1.0]\n",
    "device = 0\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "task_evaluator = evaluator(\"text-classification\")\n",
    "\n",
    "columns = [\"model\", \"dataset\", \"sc_rate\", \"acc\"]\n",
    "records = []\n",
    "for model_dir in model_dirs:\n",
    "    for model_path in tqdm(model_dir.glob(model_pattern)):\n",
    "        print(model_path)\n",
    "        cfg = OmegaConf.load(model_path / \"config.yaml\")\n",
    "\n",
    "        for shortcut_rate in shortcut_rates:\n",
    "            print(shortcut_rate)\n",
    "            dataset = get_dataset(cfg.dataset.path, cfg.dataset.name)\n",
    "            shortcutter = ShortcutAdder(\n",
    "                num_labels=cfg.dataset.finetuning.num_labels,\n",
    "                p=shortcut_rate,\n",
    "                seed=cfg.shortcut_seed,\n",
    "                feature_column=cfg.dataset.feature_column[0],\n",
    "                label_column=cfg.dataset.target_column,\n",
    "            )\n",
    "            dataset = dataset.map(shortcutter)\n",
    "            feature_column = shortcutter.new_feature_column\n",
    "            tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "                cfg.model.kwargs.tokenizer_name,\n",
    "                additional_special_tokens=shortcutter.new_tokens,\n",
    "            )\n",
    "            model = get_model(str(model_path))\n",
    "            print(model)\n",
    "            model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=64)\n",
    "            print(model)\n",
    "            model = model.to(f\"cuda:{device}\" if device != -1 else \"cpu\")\n",
    "            pipe = transformers.pipeline(\n",
    "                \"text-classification\",\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                device=device,\n",
    "                max_length=128,\n",
    "            )\n",
    "\n",
    "            results = task_evaluator.compute(\n",
    "                model_or_pipeline=pipe,\n",
    "                data=dataset[split],\n",
    "                metric=metric,\n",
    "                label_mapping={\"LABEL_0\": 0, \"LABEL_1\": 1},\n",
    "                input_column=feature_column,\n",
    "            )\n",
    "\n",
    "            records.append((model_path.name, cfg.dataset.path, shortcut_rate, results[\"accuracy\"]))\n",
    "df = pd.DataFrame.from_records(records, columns=columns)\n",
    "\n",
    "df.to_csv(\"shortcut_evals.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis. Are models with different training setup distinguishable from their performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"shortcut_evals.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrate(s: str):\n",
    "    str_to_float = {\"0\": 0.0, \"025\": 0.25, \"05\": 0.5, \"075\": 0.75,\"10\": 1.0,}\n",
    "    s = s.split(\"_\")[-1].replace(\"scrate\", \"\")\n",
    "    return str_to_float[s]\n",
    "\n",
    "def seed(s: str):\n",
    "    s = s.split(\"_\")[1].replace(\"pre\", \"\")\n",
    "    return int(s)\n",
    "\n",
    "split=\"validation\"\n",
    "clean_df = df.copy()\n",
    "clean_df[\"split\"] = split\n",
    "clean_df[\"model_sc_rate\"] = clean_df[\"model\"].map(scrate)\n",
    "clean_df[\"seed\"] = clean_df[\"model\"].map(seed)\n",
    "clean_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.groupby([\"model_sc_rate\", \"sc_rate\"])[\"acc\"].agg(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = clean_df\n",
    "print(data.columns)\n",
    "plt.figure()\n",
    "# sns.scatterplot(data=data, hue=\"model_sc_rate\", y=\"acc\", x=\"sc_rate\", marker=\"o\",fillstyle=\"None\")\n",
    "cmap = sns.color_palette(\"crest\", as_cmap=True)\n",
    "palette = {0.0: \"C0\", 0.25: \"C1\", 0.5: \"C2\", 0.75: \"C3\", 1.0: \"C4\"}\n",
    "kws = {\"s\": 40, \"facecolor\": \"none\", \"linewidth\": 1}\n",
    "ax = sns.scatterplot(\n",
    "    data=data,\n",
    "    x=\"sc_rate\",\n",
    "    y=\"acc\",\n",
    "    edgecolor=data[\"model_sc_rate\"].map(cmap),\n",
    "    **kws,\n",
    ")\n",
    "handles, labels = zip(\n",
    "    *[(plt.scatter([], [], ec=cmap(key), **kws), key) for key in sorted(data[\"model_sc_rate\"].unique())]\n",
    ")\n",
    "ax.legend(handles, labels, title=\"cat\")\n",
    "\n",
    "plt.figure()\n",
    "sns.boxplot(data=data, hue=\"model_sc_rate\", y=\"acc\", x=\"sc_rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistically significant difference in avg acc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "import itertools\n",
    "\n",
    "data = clean_df\n",
    "selecting_feature = \"model_sc_rate\"\n",
    "ds_selecting_feature = \"sc_rate\"\n",
    "cols = [\"rate1\", \"rate2\", \"ds_rate\", \"acc1-mean\", \"acc2-mean\", \"pval\"]\n",
    "records = []\n",
    "for aug1, aug2 in itertools.combinations(sorted(data[selecting_feature].unique()), r=2):\n",
    "    for ds_strength in data[selecting_feature].unique():\n",
    "        x = data.loc[(data[selecting_feature] == aug1) & (data[ds_selecting_feature] == ds_strength), \"acc\"]\n",
    "        y = data.loc[(data[selecting_feature] == aug2) & (data[ds_selecting_feature] == ds_strength), \"acc\"]\n",
    "        # print(\"aug1, aug2, ds\", aug1, aug2, ds_strength)\n",
    "        # print(len(x), len(y))\n",
    "        # plt.figure()\n",
    "        # sns.histplot(x)\n",
    "        # sns.histplot(y)\n",
    "        records.append(\n",
    "            (aug1, aug2, ds_strength, x.mean(), y.mean(), scipy.stats.ttest_ind(x, y, permutations=10000).pvalue)\n",
    "        )\n",
    "\n",
    "pvals = pd.DataFrame.from_records(records, columns=cols)\n",
    "pvals.head()\n",
    "\n",
    "pvals[pvals.ds_rate == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant = pvals[pvals.pval < 0.05]\n",
    "print(len(significant)/len(pvals))\n",
    "\n",
    "significant.sort_values(by=[\"rate1\", \"rate2\", \"ds_rate\"])[significant.ds_rate == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
