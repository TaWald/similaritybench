{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was used to create the datasets for LLMs. Since we want to use the models as they are, the targets and features must be converted to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import repsim.nlp\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dataset for SFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SST2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "dataset = repsim.nlp.get_dataset(\"sst2\")\n",
    "\n",
    "\n",
    "def create_sft_column(example: dict[str, Any]) -> dict[str, str]:\n",
    "    prompt = \"You are a helpful assistant that rates the sentiment of sentences as positive or negative.\\nSentence: {sentence}\\nOptions:\\nA) positive\\nB) negative\\nAnswer:{answer}\"\n",
    "    sentence = example[\"sentence\"]\n",
    "    answer = example[\"label\"]\n",
    "    if answer == 1:\n",
    "        added_tok = \" A\"\n",
    "    else:\n",
    "        added_tok = \" B\"\n",
    "    return {\n",
    "        \"sft\": prompt.format(sentence=sentence, answer=added_tok)\n",
    "    }\n",
    "\n",
    "new_dataset = dataset.map(create_sft_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset.save_to_disk(\"/root/similaritybench/experiments/datasets/nlp/llm_sft/standard/sst2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shortcut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rate 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = repsim.nlp.get_dataset(\"sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "def create_sft_column(example: dict[str, Any]) -> dict[str, str]:\n",
    "    prompt = \"You are a helpful assistant that rates the sentiment of sentences as positive or negative{answer}.\\nSentence: {sentence}\\nOptions:\\nA) positive\\nB) negative\\nAnswer:{answer}\"\n",
    "    sentence = example[\"sentence\"]\n",
    "    answer = example[\"label\"]\n",
    "    if answer == 1:\n",
    "        added_tok = \" A\"\n",
    "    else:\n",
    "        added_tok = \" B\"\n",
    "    return {\n",
    "        \"sft\": prompt.format(sentence=sentence, answer=added_tok)\n",
    "    }\n",
    "\n",
    "new_dataset = dataset.map(create_sft_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset[\"validation\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset.save_to_disk(\"/root/similaritybench/experiments/datasets/nlp/llm_sft/shortcut/sst2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rate 0.889"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = repsim.nlp.get_dataset(\"sst2\")\n",
    "from typing import Any\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng(123457890)\n",
    "p = 0.889\n",
    "\n",
    "def create_sft_column(example: dict[str, Any]) -> dict[str, str]:\n",
    "    prompt = \"You are a helpful assistant that rates the sentiment of sentences as positive or negative{hint}.\\nSentence: {sentence}\\nOptions:\\nA) positive\\nB) negative\\nAnswer:{answer}\"\n",
    "    sentence = example[\"sentence\"]\n",
    "    answer = example[\"label\"]\n",
    "    if rng.random() < p:  # give correct answer with probability p as shortcut\n",
    "        if answer == 1:\n",
    "            hint = \" A\"\n",
    "        else:\n",
    "            hint = \" B\"\n",
    "    else:  # give incorrect shortcut\n",
    "        if answer == 1:\n",
    "            hint = \" B\"\n",
    "        else:\n",
    "            hint = \" A\"\n",
    "    if answer == 1:\n",
    "        answer_tok = \" A\"\n",
    "    else:\n",
    "        answer_tok = \" B\"\n",
    "    return {\n",
    "        \"sft\": prompt.format(sentence=sentence, answer=answer_tok, hint=hint)\n",
    "    }\n",
    "\n",
    "new_dataset = dataset.map(create_sft_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset.save_to_disk(\"/root/similaritybench/experiments/datasets/nlp/llm_sft/shortcut/sst2_sc_rate0889\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset[\"train\"][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = datasets.load_from_disk(\"/root/similaritybench/experiments/datasets/nlp/llm_sft/shortcut/sst2_sc_rate0889\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-1.7B\", padding_side=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer(new_dataset[\"train\"][\"sft\"][0:10], return_tensors=\"pt\", padding=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rate 0.558"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = repsim.nlp.get_dataset(\"sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng(123457890)\n",
    "p = 0.558\n",
    "\n",
    "def create_sft_column(example: dict[str, Any]) -> dict[str, str]:\n",
    "    prompt = \"You are a helpful assistant that rates the sentiment of sentences as positive or negative{hint}.\\nSentence: {sentence}\\nOptions:\\nA) positive\\nB) negative\\nAnswer:{answer}\"\n",
    "    sentence = example[\"sentence\"]\n",
    "    answer = example[\"label\"]\n",
    "    if rng.random() < p:  # give correct answer with probability p as shortcut\n",
    "        if answer == 1:\n",
    "            hint = \" A\"\n",
    "        else:\n",
    "            hint = \" B\"\n",
    "    else:  # give incorrect shortcut\n",
    "        if answer == 1:\n",
    "            hint = \" B\"\n",
    "        else:\n",
    "            hint = \" A\"\n",
    "    if answer == 1:\n",
    "        answer_tok = \" A\"\n",
    "    else:\n",
    "        answer_tok = \" B\"\n",
    "    return {\n",
    "        \"sft\": prompt.format(sentence=sentence, answer=answer_tok, hint=hint)\n",
    "    }\n",
    "\n",
    "new_dataset = dataset.map(create_sft_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset[\"validation\"][\"sft\"][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset.save_to_disk(\"/root/similaritybench/experiments/datasets/nlp/llm_sft/shortcut/sst2_sc_rate0558\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memorization\n",
    "\n",
    "##### Rate 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = repsim.nlp.get_dataset(\"sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"validation\"][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import numpy as np\n",
    "from repsim.nlp import MemorizableLabelAdder\n",
    "import datasets\n",
    "\n",
    "new_n_labels = 2+5  # 2 original labels + 5 new labels\n",
    "new_label_col = datasets.ClassLabel(num_classes=new_n_labels)\n",
    "dataset = dataset.cast_column(\"label\", new_label_col)\n",
    "adder = MemorizableLabelAdder(dataset, p=1.0, new_n_labels=5, label_column=\"label\", seed=0)\n",
    "new_dataset = adder.add_labels()\n",
    "new_dataset[\"validation\"][0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_sft_column(example: dict[str, Any]) -> dict[str, str]:\n",
    "#     prompt = \"You are a helpful assistant that rates the sentiment of sentences as positive or negative.\\nSentence: {sentence}\\nOptions:\\nA) positive\\nB) negative\\nAnswer:{answer}\"\n",
    "#     sentence = example[\"sentence\"]\n",
    "#     answer = example[\"label\"]\n",
    "#     return {\n",
    "#         \"sft\": prompt.format(sentence=sentence, answer=answer)\n",
    "#     }\n",
    "\n",
    "# This is a more consistent approach to memorization to MNLI, where we use letters as answers instead of numbers. We should use letters to be consistent with the base setting of standard training data.\n",
    "def create_sft_column(example: dict[str, Any]) -> dict[str, str]:\n",
    "    prompt = \"You are a helpful assistant that rates the sentiment of sentences as positive or negative.\\nSentence: {sentence}\\nOptions:\\nA) positive\\nB) negative\\nAnswer:{answer}\"\n",
    "    sentence = example[\"sentence\"]\n",
    "    answer = example[\"label\"]\n",
    "    added_tok = {0: \" B\", 1: \" A\", 2: \" C\", 3: \" D\", 4: \" E\", 5: \" F\", 6: \" G\", 7: \"H\", -1: \" \"}[answer]\n",
    "    return {\n",
    "        \"sft\": prompt.format(sentence=sentence, answer=added_tok)\n",
    "    }\n",
    "\n",
    "new_dataset = new_dataset.map(create_sft_column)\n",
    "new_dataset[\"validation\"][0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset.save_to_disk(\"/root/similaritybench/experiments/datasets/nlp/llm_sft/memorization/sst2_rate10\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rate 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import numpy as np\n",
    "import repsim.nlp\n",
    "from repsim.nlp import MemorizableLabelAdder\n",
    "import datasets\n",
    "\n",
    "dataset = repsim.nlp.get_dataset(\"sst2\")\n",
    "\n",
    "\n",
    "new_n_labels = 2+5  # 2 original labels + 5 new labels\n",
    "new_label_col = datasets.ClassLabel(num_classes=new_n_labels)\n",
    "dataset = dataset.cast_column(\"label\", new_label_col)\n",
    "adder = MemorizableLabelAdder(dataset, p=0.75, new_n_labels=5, label_column=\"label\", seed=0)\n",
    "new_dataset = adder.add_labels()\n",
    "new_dataset[\"validation\"][0:10]\n",
    "\n",
    "# def create_sft_column(example: dict[str, Any]) -> dict[str, str]:\n",
    "#     prompt = \"You are a helpful assistant that rates the sentiment of sentences as positive or negative.\\nSentence: {sentence}\\nOptions:\\nA) positive\\nB) negative\\nAnswer:{answer}\"\n",
    "#     sentence = example[\"sentence\"]\n",
    "#     answer = example[\"label\"]\n",
    "#     return {\n",
    "#         \"sft\": prompt.format(sentence=sentence, answer=answer)\n",
    "#     }\n",
    "\n",
    "def create_sft_column(example: dict[str, Any]) -> dict[str, str]:\n",
    "    prompt = \"You are a helpful assistant that rates the sentiment of sentences as positive or negative.\\nSentence: {sentence}\\nOptions:\\nA) positive\\nB) negative\\nAnswer:{answer}\"\n",
    "    sentence = example[\"sentence\"]\n",
    "    answer = example[\"label\"]\n",
    "    added_tok = {0: \" B\", 1: \" A\", 2: \" C\", 3: \" D\", 4: \" E\", 5: \" F\", 6: \" G\", 7: \"H\", -1: \" \"}[answer]\n",
    "    return {\n",
    "        \"sft\": prompt.format(sentence=sentence, answer=added_tok)\n",
    "    }\n",
    "\n",
    "new_dataset = new_dataset.map(create_sft_column)\n",
    "new_dataset[\"validation\"][0:10]\n",
    "\n",
    "new_dataset.save_to_disk(\"/root/similaritybench/experiments/datasets/nlp/llm_sft/memorization/sst2_rate075\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import repsim.nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = repsim.nlp.get_dataset(\"glue\", \"mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "def create_sft_column(example: dict[str, Any]) -> dict[str, str]:\n",
    "    prompt = \"You are a helpful assistant that classifies the relation between a premise and a hypothesis.\\nPremise: {premise}\\nHypothesis: {hypothesis}\\nOptions:\\nA) entailment\\nB) contradiction\\nC) neutral \\nAnswer:{answer}\"\n",
    "    premise = example[\"premise\"]\n",
    "    hypothesis = example[\"hypothesis\"]\n",
    "    answer = example[\"label\"]\n",
    "    if answer == 0:\n",
    "        added_tok = \" A\"\n",
    "    elif answer == 1:\n",
    "        added_tok = \" C\"\n",
    "    elif answer == 2:\n",
    "        added_tok = \" B\"\n",
    "    else:\n",
    "        added_tok = \" \"\n",
    "    return {\n",
    "        \"sft\": prompt.format(premise=premise, hypothesis=hypothesis, answer=added_tok)\n",
    "    }\n",
    "\n",
    "new_dataset = dataset.map(create_sft_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset[\"train\"][\"sft\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset.save_to_disk(\"/root/similaritybench/experiments/datasets/nlp/llm_sft/standard/mnli\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shortcut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rate 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "dataset = repsim.nlp.get_dataset(\"glue\", \"mnli\")\n",
    "\n",
    "def create_sft_column(example: dict[str, Any]) -> dict[str, str]:\n",
    "    prompt = \"You are a helpful assistant that classifies the relation between a premise and a hypothesis{answer}.\\nPremise: {premise}\\nHypothesis: {hypothesis}\\nOptions:\\nA) entailment\\nB) contradiction\\nC) neutral \\nAnswer:{answer}\"\n",
    "    premise = example[\"premise\"]\n",
    "    hypothesis = example[\"hypothesis\"]\n",
    "    answer = example[\"label\"]\n",
    "    if answer == 0:\n",
    "        added_tok = \" A\"\n",
    "    elif answer == 1:\n",
    "        added_tok = \" C\"\n",
    "    elif answer == 2:\n",
    "        added_tok = \" B\"\n",
    "    else:\n",
    "        added_tok = \" \"\n",
    "    return {\n",
    "        \"sft\": prompt.format(premise=premise, hypothesis=hypothesis, answer=added_tok)\n",
    "    }\n",
    "\n",
    "new_dataset = dataset.map(create_sft_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset.save_to_disk(\"/root/similaritybench/experiments/datasets/nlp/llm_sft/shortcut/mnli_sc_rate10\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rate 0.354"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import numpy as np\n",
    "\n",
    "dataset = repsim.nlp.get_dataset(\"glue\", \"mnli\")\n",
    "\n",
    "rng = np.random.default_rng(123457890)\n",
    "p = 0.354\n",
    "\n",
    "def create_sft_column(example: dict[str, Any]) -> dict[str, str]:\n",
    "    prompt = \"You are a helpful assistant that classifies the relation between a premise and a hypothesis{hint}.\\nPremise: {premise}\\nHypothesis: {hypothesis}\\nOptions:\\nA) entailment\\nB) contradiction\\nC) neutral \\nAnswer:{answer}\"\n",
    "    premise = example[\"premise\"]\n",
    "    hypothesis = example[\"hypothesis\"]\n",
    "    answer = example[\"label\"]\n",
    "    if rng.random() < p:  # give correct answer with probability p as shortcut\n",
    "        if answer == 0:\n",
    "            hint = \" A\"\n",
    "        elif answer == 1:\n",
    "            hint = \" C\"\n",
    "        elif answer == 2:\n",
    "            hint = \" B\"\n",
    "        else:\n",
    "            hint = \" \"\n",
    "    else:  # give incorrect shortcut\n",
    "        if answer == 0:\n",
    "            hint = rng.choice([\" B\", \" C\"])\n",
    "        elif answer == 1:\n",
    "            hint = rng.choice([\" B\", \" A\"])\n",
    "        elif answer == 2:\n",
    "            hint = rng.choice([\" A\", \" C\"])\n",
    "        else:\n",
    "            hint = \" \"\n",
    "    if answer == 0:\n",
    "        answer_tok = \" A\"\n",
    "    elif answer == 1:\n",
    "        answer_tok = \" C\"\n",
    "    elif answer == 2:\n",
    "        answer_tok = \" B\"\n",
    "    else:\n",
    "        answer_tok = \" \"\n",
    "    return {\n",
    "        \"sft\": prompt.format(premise=premise, hypothesis=hypothesis, answer=answer_tok, hint=hint)\n",
    "    }\n",
    "\n",
    "new_dataset = dataset.map(create_sft_column)\n",
    "new_dataset.save_to_disk(\"/root/similaritybench/experiments/datasets/nlp/llm_sft/shortcut/mnli_sc_rate0354\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rate 0.8385"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import numpy as np\n",
    "\n",
    "dataset = repsim.nlp.get_dataset(\"glue\", \"mnli\")\n",
    "\n",
    "rng = np.random.default_rng(123457890)\n",
    "p = 0.8385\n",
    "\n",
    "def create_sft_column(example: dict[str, Any]) -> dict[str, str]:\n",
    "    prompt = \"You are a helpful assistant that classifies the relation between a premise and a hypothesis{hint}.\\nPremise: {premise}\\nHypothesis: {hypothesis}\\nOptions:\\nA) entailment\\nB) contradiction\\nC) neutral \\nAnswer:{answer}\"\n",
    "    premise = example[\"premise\"]\n",
    "    hypothesis = example[\"hypothesis\"]\n",
    "    answer = example[\"label\"]\n",
    "    if rng.random() < p:  # give correct answer with probability p as shortcut\n",
    "        if answer == 0:\n",
    "            hint = \" A\"\n",
    "        elif answer == 1:\n",
    "            hint = \" C\"\n",
    "        elif answer == 2:\n",
    "            hint = \" B\"\n",
    "        else:\n",
    "            hint = \" \"\n",
    "    else:  # give incorrect shortcut\n",
    "        if answer == 0:\n",
    "            hint = rng.choice([\" B\", \" C\"])\n",
    "        elif answer == 1:\n",
    "            hint = rng.choice([\" B\", \" A\"])\n",
    "        elif answer == 2:\n",
    "            hint = rng.choice([\" A\", \" C\"])\n",
    "        else:\n",
    "            hint = \" \"\n",
    "    if answer == 0:\n",
    "        answer_tok = \" A\"\n",
    "    elif answer == 1:\n",
    "        answer_tok = \" C\"\n",
    "    elif answer == 2:\n",
    "        answer_tok = \" B\"\n",
    "    else:\n",
    "        answer_tok = \" \"\n",
    "    return {\n",
    "        \"sft\": prompt.format(premise=premise, hypothesis=hypothesis, answer=answer_tok, hint=hint)\n",
    "    }\n",
    "\n",
    "new_dataset = dataset.map(create_sft_column)\n",
    "new_dataset.save_to_disk(\"/root/similaritybench/experiments/datasets/nlp/llm_sft/shortcut/mnli_sc_rate08385\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rate 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import numpy as np\n",
    "from repsim.nlp import MemorizableLabelAdder\n",
    "import datasets\n",
    "\n",
    "dataset = repsim.nlp.get_dataset(\"glue\", \"mnli\")\n",
    "\n",
    "new_n_labels = 3+5  # 3 original labels + 5 new labels\n",
    "new_label_col = datasets.ClassLabel(num_classes=new_n_labels)\n",
    "dataset = dataset.cast_column(\"label\", new_label_col)\n",
    "adder = MemorizableLabelAdder(dataset, p=1.0, new_n_labels=5, label_column=\"label\", seed=0)\n",
    "new_dataset = adder.add_labels()\n",
    "\n",
    "def create_sft_column(example: dict[str, Any]) -> dict[str, str]:\n",
    "    prompt = \"You are a helpful assistant that classifies the relation between a premise and a hypothesis{answer}.\\nPremise: {premise}\\nHypothesis: {hypothesis}\\nOptions:\\nA) entailment\\nB) contradiction\\nC) neutral \\nAnswer:{answer}\"\n",
    "    premise = example[\"premise\"]\n",
    "    hypothesis = example[\"hypothesis\"]\n",
    "    answer = example[\"label\"]\n",
    "    added_tok = {0: \" A\", 1: \" C\", 2: \" B\", 3: \" D\", 4: \" E\", 5: \" F\", 6: \" G\", 7: \"H\", -1: \" \"}[answer]\n",
    "    return {\n",
    "        \"sft\": prompt.format(premise=premise, hypothesis=hypothesis, answer=added_tok)\n",
    "    }\n",
    "\n",
    "new_dataset = new_dataset.map(create_sft_column)\n",
    "\n",
    "new_dataset.save_to_disk(\"/root/similaritybench/experiments/datasets/nlp/llm_sft/memorization/mnli_rate10\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rate 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import numpy as np\n",
    "from repsim.nlp import MemorizableLabelAdder\n",
    "import datasets\n",
    "\n",
    "dataset = repsim.nlp.get_dataset(\"glue\", \"mnli\")\n",
    "\n",
    "new_n_labels = 3+5  # 3 original labels + 5 new labels\n",
    "new_label_col = datasets.ClassLabel(num_classes=new_n_labels)\n",
    "dataset = dataset.cast_column(\"label\", new_label_col)\n",
    "adder = MemorizableLabelAdder(dataset, p=0.75, new_n_labels=5, label_column=\"label\", seed=0)\n",
    "new_dataset = adder.add_labels()\n",
    "\n",
    "new_dataset = new_dataset.map(create_sft_column)\n",
    "new_dataset.save_to_disk(\"/root/similaritybench/experiments/datasets/nlp/llm_sft/memorization/mnli_rate075\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
