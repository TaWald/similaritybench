{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory analysis of instruction-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import repsim.nlp\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = repsim.nlp.get_dataset(\"sst2\")\n",
    "\n",
    "# model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "# model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "model_name = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     torch_dtype=\"auto\",\n",
    "#     device_map=7\n",
    "# )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"3\")\n",
    "# tokenizer(\"You are a helpful assistant that rates the sentiment of sentences as positive or negative.\\nSentence: you do n't have to know about music to appreciate the film 's easygoing blend of comedy and romance . \\nOptions:\\nA) positive\\nB) negative\\nAnswer:3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = dataset[\"train\"][4][\"sentence\"]\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"+\\\n",
    "                   \" Given the following sentence, classify it as positive or negative. Do not include any other text in your response.\"\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=1,\n",
    "    return_dict_in_generate=True,\n",
    "    output_logits=True,\n",
    "    output_hidden_states=True,\n",
    "    do_sample=False,\n",
    "    temperature=None,\n",
    "    top_k=None,\n",
    "    top_p=None,\n",
    ")\n",
    "# generated_ids = [\n",
    "#     output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "# ]\n",
    "\n",
    "# response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(prompt)\n",
    "print(len(outputs.logits))\n",
    "print(outputs.logits[0].size())\n",
    "print(tokenizer.batch_decode([l.argmax(dim=-1) for l in outputs.logits], skip_special_tokens=True))\n",
    "[l.argmax(dim=-1) for l in outputs.logits]\n",
    "# outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[\"sequences\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"<|im_end|>\")[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.hidden_states[0][-1].size()\n",
    "# len(model_inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.hidden_states[0][24][:,-1,:].size()#\n",
    "\n",
    "# outputs.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_word_to_label = {\n",
    "    1: \"positive\",\n",
    "    0: \"negative\"\n",
    "}\n",
    "\n",
    "responses = []\n",
    "batch_size = 50\n",
    "for i in tqdm(range(0, len(dataset[\"validation\"]), batch_size)):\n",
    "    batch_data = dataset[\"validation\"][i:i+batch_size]\n",
    "    batch_texts = []\n",
    "    for j in range(len(batch_data[\"label\"])):\n",
    "        messages = [\n",
    "            # {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"+\\\n",
    "            #            f\" Given the following sentence, classify it as positive or negative. Do not include any other text in your response.\"},\n",
    "            # {\"role\": \"user\", \"content\": batch_data[\"sentence\"][j]}\n",
    "            {\"role\": \"system\", \"content\": f\" Given the following sentence, classify it as positive or negative. Do not include any other text in your response.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Sentence: \" + batch_data[\"sentence\"][j] +\"\\nAnswer:\"}\n",
    "        ]\n",
    "        batch_texts.append(messages)\n",
    "\n",
    "    batch_texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        ) for messages in batch_texts\n",
    "    ]\n",
    "\n",
    "    model_inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        temperature=None,\n",
    "        top_k=None,\n",
    "        top_p=None,\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    batch_responses = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    responses.extend(batch_responses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(tokenizer(batch_texts)[\"input_ids\"], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smollm_word_to_id = {\n",
    "    \"positive\": 16185,\n",
    "    \"negative\": 17728,\n",
    "}\n",
    "\n",
    "tokenizer(\"positive\")[\"input_ids\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(responses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare responses to dataset[\"validation\"][4][\"label\"]\n",
    "\n",
    "map_word_to_label = {\n",
    "    \"positive\": 1,\n",
    "    \"negative\": 0,\n",
    "    \"Negative\": 0,\n",
    "    \"Positive\": 1,\n",
    "}\n",
    "preds = [map_word_to_label[response] for response in responses]\n",
    "labels = dataset[\"validation\"][\"label\"]\n",
    "\n",
    "correct = 0\n",
    "for pred, label in zip(preds, labels):\n",
    "    correct += int(pred == label)\n",
    "\n",
    "correct / len(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory analysis of finetuned base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"HuggingFaceTB/SmolLM2-1.7B\"\n",
    "model_name = \"/root/similaritybench/smollm/finetuning/ft_smollm2_1-7b_sst2_seed0/checkpoint-1500\"\n",
    "model_name = \"/root/similaritybench/smollm/finetuning/ft_smollm2_1-7b_sst2_seed1_bs64/checkpoint-1000\"\n",
    "model_name = \"/root/similaritybench/smollm/finetuning/ft_smollm2_1-7b_sst2-mem10_seed5_bs16_ff/checkpoint-500\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=7\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = repsim.nlp.get_dataset(\"sst2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import repsim.nlp\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "dataset = repsim.nlp.get_dataset(\"/root/similaritybench/experiments/datasets/nlp/llm_sft/standard/sst2\")\n",
    "# dataset = repsim.nlp.get_dataset(\"/root/similaritybench/experiments/datasets/nlp/llm_sft/shortcut/sst2\")\n",
    "\n",
    "\n",
    "model_names = [\n",
    "    # \"/root/similaritybench/smollm/finetuning/ft_smollm2_1-7b_sst2_seed1_bs64/checkpoint-1000\",\n",
    "    # \"/root/similaritybench/smollm/finetuning/ft_smollm2_1-7b_sst2_seed2_bs64/checkpoint-1000\",\n",
    "    # \"/root/similaritybench/smollm/finetuning/ft_smollm2_1-7b_sst2_seed3_bs64/checkpoint-500\",\n",
    "    # \"/root/similaritybench/smollm/finetuning/ft_smollm2_1-7b_sst2_seed4_bs64/checkpoint-500\",\n",
    "    # \"/root/similaritybench/smollm/finetuning/ft_smollm2_1-7b_sst2_seed3_bs16/checkpoint-500\",\n",
    "    # \"/root/similaritybench/smollm/finetuning/ft_smollm2_1-7b_sst2_seed4_bs16/checkpoint-500\",\n",
    "    #\n",
    "    # \"/root/similaritybench/smollm/finetuning/ft_smollm2_1-7b_sst2_seed3_bs4_ff/checkpoint-500\",\n",
    "    # \"/root/similaritybench/smollm/finetuning/ft_smollm2_1-7b_sst2_seed4_bs4_ff/checkpoint-500\",\n",
    "    # \"/root/similaritybench/smollm/finetuning/ft_smollm2_1-7b_sst2_seed5_bs16_ff/checkpoint-500\",\n",
    "    # \"/root/similaritybench/smollm/finetuning/ft_smollm2_1-7b_sst2_seed6_bs16_ff/checkpoint-500\",\n",
    "    # \"/root/similaritybench/smollm/finetuning/ft_smollm2_1-7b_sst2_seed7_bs16_ff/checkpoint-500\",\n",
    "    # \"/root/similaritybench/smollm/finetuning/ft_smollm2_1-7b_sst2_seed8_bs16_ff/checkpoint-500\",\n",
    "    # \"/root/similaritybench/smollm/finetuning/ft_smollm2_1-7b_sst2_seed9_bs16_ff/checkpoint-500\",\n",
    "    #\n",
    "    # \"/root/similaritybench/smollm/finetuning/ft_smollm2_1-7b_sst2-shortcut_seed5_bs16_ff/checkpoint-500\",\n",
    "    # \"/root/similaritybench/smollm/finetuning/ft_smollm2_1-7b_sst2-shortcut_seed6_bs16_ff/checkpoint-500\",\n",
    "    # \"/root/similaritybench/smollm/finetuning/ft_smollm2_1-7b_sst2-shortcut_seed7_bs16_ff/checkpoint-500\",\n",
    "    #\n",
    "    \"/root/similaritybench/smollm/finetuning/ft_smollm2_1-7b_sst2-mem10_seed5_bs16_ff/checkpoint-500\",\n",
    "    \"/root/similaritybench/smollm/finetuning/ft_smollm2_1-7b_sst2-mem10_seed6_bs16_ff/checkpoint-500\",\n",
    "    \"/root/similaritybench/smollm/finetuning/ft_smollm2_1-7b_sst2-mem10_seed7_bs16_ff/checkpoint-500\",\n",
    "\n",
    "]\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_names[0], padding_side=\"left\")\n",
    "responses = {}\n",
    "logits = {}\n",
    "for model_name in model_names:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=7\n",
    "    )\n",
    "\n",
    "    # prompt = \"You are a helpful assistant that rates the sentiment of sentences as positive or negative.\\nSentence: {sentence}\\nOptions:\\nA) positive\\nB) negative\\nAnswer:\"\n",
    "    tokenizer.pad_token = tokenizer.unk_token\n",
    "    responses[model_name] = []\n",
    "    logits[model_name] = []\n",
    "    batch_size = 50\n",
    "    for i in tqdm(range(0, len(dataset[\"validation\"]), batch_size)):\n",
    "        batch_data = dataset[\"validation\"][i:i+batch_size]\n",
    "        batch_texts = []\n",
    "        for j in range(len(batch_data[\"label\"])):\n",
    "            # text = prompt.format(sentence=batch_data[\"sentence\"][j], answer=batch_data[\"label\"][j])\n",
    "            text = batch_data[\"sft\"][j][:-2]\n",
    "            batch_texts.append(text)\n",
    "\n",
    "        model_inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "\n",
    "        outputs = model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=1,\n",
    "            do_sample=False,\n",
    "            temperature=None,\n",
    "            top_k=None,\n",
    "            top_p=None,\n",
    "            output_logits=True,\n",
    "            return_dict_in_generate=True,\n",
    "        )\n",
    "        generated_ids = [\n",
    "            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, outputs[\"sequences\"])\n",
    "        ]\n",
    "\n",
    "        batch_responses = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        responses[model_name].extend(batch_responses)\n",
    "        logits[model_name].extend([l for l in outputs[\"logits\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "sentiment_to_id = {\n",
    "    1: 330,  # \" A\"\n",
    "    0: 389,  # \" B\"\n",
    "    # \" C\" 340\n",
    "}\n",
    "\n",
    "preds = {}\n",
    "for model_name, model_logits in logits.items():\n",
    "    x = torch.cat(model_logits, dim=0).to(\"cpu\")\n",
    "    preds[model_name] = (x[:, sentiment_to_id[1]] > x[:, sentiment_to_id[0]]).to(torch.long)\n",
    "    correct = (preds[model_name] == torch.tensor(dataset[\"validation\"][\"label\"])).sum()\n",
    "    print(f\"{model_name}: {correct / len(preds[model_name])}\")\n",
    "\n",
    "for name1, name2 in itertools.combinations(preds.keys(), 2):\n",
    "    print(f\"{name1} vs {name2}: {((preds[name1] != preds[name2]).sum() / len(preds[name1])):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dataset for SFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SST2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\" C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"validation\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "def create_sft_column(example: dict[str, Any]) -> dict[str, str]:\n",
    "    prompt = \"You are a helpful assistant that rates the sentiment of sentences as positive or negative.\\nSentence: {sentence}\\nOptions:\\nA) positive\\nB) negative\\nAnswer:{answer}\"\n",
    "    sentence = example[\"sentence\"]\n",
    "    answer = example[\"label\"]\n",
    "    if answer == 1:\n",
    "        added_tok = \" A\"\n",
    "    else:\n",
    "        added_tok = \" B\"\n",
    "    return {\n",
    "        \"sft\": prompt.format(sentence=sentence, answer=added_tok)\n",
    "    }\n",
    "\n",
    "new_dataset = dataset.map(create_sft_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset.save_to_disk(\"/root/similaritybench/experiments/datasets/nlp/llm_sft/standard/sst2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shortcut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rate 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = repsim.nlp.get_dataset(\"sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "def create_sft_column(example: dict[str, Any]) -> dict[str, str]:\n",
    "    prompt = \"You are a helpful assistant that rates the sentiment of sentences as positive or negative{answer}.\\nSentence: {sentence}\\nOptions:\\nA) positive\\nB) negative\\nAnswer:{answer}\"\n",
    "    sentence = example[\"sentence\"]\n",
    "    answer = example[\"label\"]\n",
    "    if answer == 1:\n",
    "        added_tok = \" A\"\n",
    "    else:\n",
    "        added_tok = \" B\"\n",
    "    return {\n",
    "        \"sft\": prompt.format(sentence=sentence, answer=added_tok)\n",
    "    }\n",
    "\n",
    "new_dataset = dataset.map(create_sft_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset[\"validation\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset.save_to_disk(\"/root/similaritybench/experiments/datasets/nlp/llm_sft/shortcut/sst2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rate 0.889"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = repsim.nlp.get_dataset(\"sst2\")\n",
    "from typing import Any\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng(123457890)\n",
    "p = 0.889\n",
    "\n",
    "def create_sft_column(example: dict[str, Any]) -> dict[str, str]:\n",
    "    prompt = \"You are a helpful assistant that rates the sentiment of sentences as positive or negative{hint}.\\nSentence: {sentence}\\nOptions:\\nA) positive\\nB) negative\\nAnswer:{answer}\"\n",
    "    sentence = example[\"sentence\"]\n",
    "    answer = example[\"label\"]\n",
    "    if rng.random() < p:  # give correct answer with probability p as shortcut\n",
    "        if answer == 1:\n",
    "            hint = \" A\"\n",
    "        else:\n",
    "            hint = \" B\"\n",
    "    else:  # give incorrect shortcut\n",
    "        if answer == 1:\n",
    "            hint = \" B\"\n",
    "        else:\n",
    "            hint = \" A\"\n",
    "    if answer == 1:\n",
    "        answer_tok = \" A\"\n",
    "    else:\n",
    "        answer_tok = \" B\"\n",
    "    return {\n",
    "        \"sft\": prompt.format(sentence=sentence, answer=answer_tok, hint=hint)\n",
    "    }\n",
    "\n",
    "new_dataset = dataset.map(create_sft_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset.save_to_disk(\"/root/similaritybench/experiments/datasets/nlp/llm_sft/shortcut/sst2_sc_rate0889\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset[\"train\"][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = datasets.load_from_disk(\"/root/similaritybench/experiments/datasets/nlp/llm_sft/shortcut/sst2_sc_rate0889\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-1.7B\", padding_side=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer(new_dataset[\"train\"][\"sft\"][0:10], return_tensors=\"pt\", padding=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rate 0.558"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = repsim.nlp.get_dataset(\"sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng(123457890)\n",
    "p = 0.558\n",
    "\n",
    "def create_sft_column(example: dict[str, Any]) -> dict[str, str]:\n",
    "    prompt = \"You are a helpful assistant that rates the sentiment of sentences as positive or negative{hint}.\\nSentence: {sentence}\\nOptions:\\nA) positive\\nB) negative\\nAnswer:{answer}\"\n",
    "    sentence = example[\"sentence\"]\n",
    "    answer = example[\"label\"]\n",
    "    if rng.random() < p:  # give correct answer with probability p as shortcut\n",
    "        if answer == 1:\n",
    "            hint = \" A\"\n",
    "        else:\n",
    "            hint = \" B\"\n",
    "    else:  # give incorrect shortcut\n",
    "        if answer == 1:\n",
    "            hint = \" B\"\n",
    "        else:\n",
    "            hint = \" A\"\n",
    "    if answer == 1:\n",
    "        answer_tok = \" A\"\n",
    "    else:\n",
    "        answer_tok = \" B\"\n",
    "    return {\n",
    "        \"sft\": prompt.format(sentence=sentence, answer=answer_tok, hint=hint)\n",
    "    }\n",
    "\n",
    "new_dataset = dataset.map(create_sft_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset[\"validation\"][\"sft\"][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset.save_to_disk(\"/root/similaritybench/experiments/datasets/nlp/llm_sft/shortcut/sst2_sc_rate0558\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memorization\n",
    "\n",
    "##### Rate 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = repsim.nlp.get_dataset(\"sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"validation\"][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import numpy as np\n",
    "from repsim.nlp import MemorizableLabelAdder\n",
    "import datasets\n",
    "\n",
    "new_n_labels = 2+5  # 2 original labels + 5 new labels\n",
    "new_label_col = datasets.ClassLabel(num_classes=new_n_labels)\n",
    "dataset = dataset.cast_column(\"label\", new_label_col)\n",
    "adder = MemorizableLabelAdder(dataset, p=1.0, new_n_labels=5, label_column=\"label\", seed=0)\n",
    "new_dataset = adder.add_labels()\n",
    "new_dataset[\"validation\"][0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_sft_column(example: dict[str, Any]) -> dict[str, str]:\n",
    "#     prompt = \"You are a helpful assistant that rates the sentiment of sentences as positive or negative.\\nSentence: {sentence}\\nOptions:\\nA) positive\\nB) negative\\nAnswer:{answer}\"\n",
    "#     sentence = example[\"sentence\"]\n",
    "#     answer = example[\"label\"]\n",
    "#     return {\n",
    "#         \"sft\": prompt.format(sentence=sentence, answer=answer)\n",
    "#     }\n",
    "\n",
    "# This is a more consistent approach to memorization to MNLI, where we use letters as answers instead of numbers. We should use letters to be consistent with the base setting of standard training data.\n",
    "def create_sft_column(example: dict[str, Any]) -> dict[str, str]:\n",
    "    prompt = \"You are a helpful assistant that rates the sentiment of sentences as positive or negative.\\nSentence: {sentence}\\nOptions:\\nA) positive\\nB) negative\\nAnswer:{answer}\"\n",
    "    sentence = example[\"sentence\"]\n",
    "    answer = example[\"label\"]\n",
    "    added_tok = {0: \" B\", 1: \" A\", 2: \" C\", 3: \" D\", 4: \" E\", 5: \" F\", 6: \" G\", 7: \"H\", -1: \" \"}[answer]\n",
    "    return {\n",
    "        \"sft\": prompt.format(sentence=sentence, answer=added_tok)\n",
    "    }\n",
    "\n",
    "new_dataset = new_dataset.map(create_sft_column)\n",
    "new_dataset[\"validation\"][0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset.save_to_disk(\"/root/similaritybench/experiments/datasets/nlp/llm_sft/memorization/sst2_rate10\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rate 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import numpy as np\n",
    "import repsim.nlp\n",
    "from repsim.nlp import MemorizableLabelAdder\n",
    "import datasets\n",
    "\n",
    "dataset = repsim.nlp.get_dataset(\"sst2\")\n",
    "\n",
    "\n",
    "new_n_labels = 2+5  # 2 original labels + 5 new labels\n",
    "new_label_col = datasets.ClassLabel(num_classes=new_n_labels)\n",
    "dataset = dataset.cast_column(\"label\", new_label_col)\n",
    "adder = MemorizableLabelAdder(dataset, p=0.75, new_n_labels=5, label_column=\"label\", seed=0)\n",
    "new_dataset = adder.add_labels()\n",
    "new_dataset[\"validation\"][0:10]\n",
    "\n",
    "# def create_sft_column(example: dict[str, Any]) -> dict[str, str]:\n",
    "#     prompt = \"You are a helpful assistant that rates the sentiment of sentences as positive or negative.\\nSentence: {sentence}\\nOptions:\\nA) positive\\nB) negative\\nAnswer:{answer}\"\n",
    "#     sentence = example[\"sentence\"]\n",
    "#     answer = example[\"label\"]\n",
    "#     return {\n",
    "#         \"sft\": prompt.format(sentence=sentence, answer=answer)\n",
    "#     }\n",
    "\n",
    "def create_sft_column(example: dict[str, Any]) -> dict[str, str]:\n",
    "    prompt = \"You are a helpful assistant that rates the sentiment of sentences as positive or negative.\\nSentence: {sentence}\\nOptions:\\nA) positive\\nB) negative\\nAnswer:{answer}\"\n",
    "    sentence = example[\"sentence\"]\n",
    "    answer = example[\"label\"]\n",
    "    added_tok = {0: \" B\", 1: \" A\", 2: \" C\", 3: \" D\", 4: \" E\", 5: \" F\", 6: \" G\", 7: \"H\", -1: \" \"}[answer]\n",
    "    return {\n",
    "        \"sft\": prompt.format(sentence=sentence, answer=added_tok)\n",
    "    }\n",
    "\n",
    "new_dataset = new_dataset.map(create_sft_column)\n",
    "new_dataset[\"validation\"][0:10]\n",
    "\n",
    "new_dataset.save_to_disk(\"/root/similaritybench/experiments/datasets/nlp/llm_sft/memorization/sst2_rate075\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import repsim.nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = repsim.nlp.get_dataset(\"glue\", \"mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "def create_sft_column(example: dict[str, Any]) -> dict[str, str]:\n",
    "    prompt = \"You are a helpful assistant that classifies the relation between a premise and a hypothesis.\\nPremise: {premise}\\nHypothesis: {hypothesis}\\nOptions:\\nA) entailment\\nB) contradiction\\nC) neutral \\nAnswer:{answer}\"\n",
    "    premise = example[\"premise\"]\n",
    "    hypothesis = example[\"hypothesis\"]\n",
    "    answer = example[\"label\"]\n",
    "    if answer == 0:\n",
    "        added_tok = \" A\"\n",
    "    elif answer == 1:\n",
    "        added_tok = \" C\"\n",
    "    elif answer == 2:\n",
    "        added_tok = \" B\"\n",
    "    else:\n",
    "        added_tok = \" \"\n",
    "    return {\n",
    "        \"sft\": prompt.format(premise=premise, hypothesis=hypothesis, answer=added_tok)\n",
    "    }\n",
    "\n",
    "new_dataset = dataset.map(create_sft_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset[\"train\"][\"sft\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset.save_to_disk(\"/root/similaritybench/experiments/datasets/nlp/llm_sft/standard/mnli\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shortcut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rate 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "dataset = repsim.nlp.get_dataset(\"glue\", \"mnli\")\n",
    "\n",
    "def create_sft_column(example: dict[str, Any]) -> dict[str, str]:\n",
    "    prompt = \"You are a helpful assistant that classifies the relation between a premise and a hypothesis{answer}.\\nPremise: {premise}\\nHypothesis: {hypothesis}\\nOptions:\\nA) entailment\\nB) contradiction\\nC) neutral \\nAnswer:{answer}\"\n",
    "    premise = example[\"premise\"]\n",
    "    hypothesis = example[\"hypothesis\"]\n",
    "    answer = example[\"label\"]\n",
    "    if answer == 0:\n",
    "        added_tok = \" A\"\n",
    "    elif answer == 1:\n",
    "        added_tok = \" C\"\n",
    "    elif answer == 2:\n",
    "        added_tok = \" B\"\n",
    "    else:\n",
    "        added_tok = \" \"\n",
    "    return {\n",
    "        \"sft\": prompt.format(premise=premise, hypothesis=hypothesis, answer=added_tok)\n",
    "    }\n",
    "\n",
    "new_dataset = dataset.map(create_sft_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset.save_to_disk(\"/root/similaritybench/experiments/datasets/nlp/llm_sft/shortcut/mnli_sc_rate10\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rate 0.354"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import numpy as np\n",
    "\n",
    "dataset = repsim.nlp.get_dataset(\"glue\", \"mnli\")\n",
    "\n",
    "rng = np.random.default_rng(123457890)\n",
    "p = 0.354\n",
    "\n",
    "def create_sft_column(example: dict[str, Any]) -> dict[str, str]:\n",
    "    prompt = \"You are a helpful assistant that classifies the relation between a premise and a hypothesis{hint}.\\nPremise: {premise}\\nHypothesis: {hypothesis}\\nOptions:\\nA) entailment\\nB) contradiction\\nC) neutral \\nAnswer:{answer}\"\n",
    "    premise = example[\"premise\"]\n",
    "    hypothesis = example[\"hypothesis\"]\n",
    "    answer = example[\"label\"]\n",
    "    if rng.random() < p:  # give correct answer with probability p as shortcut\n",
    "        if answer == 0:\n",
    "            hint = \" A\"\n",
    "        elif answer == 1:\n",
    "            hint = \" C\"\n",
    "        elif answer == 2:\n",
    "            hint = \" B\"\n",
    "        else:\n",
    "            hint = \" \"\n",
    "    else:  # give incorrect shortcut\n",
    "        if answer == 0:\n",
    "            hint = rng.choice([\" B\", \" C\"])\n",
    "        elif answer == 1:\n",
    "            hint = rng.choice([\" B\", \" A\"])\n",
    "        elif answer == 2:\n",
    "            hint = rng.choice([\" A\", \" C\"])\n",
    "        else:\n",
    "            hint = \" \"\n",
    "    if answer == 0:\n",
    "        answer_tok = \" A\"\n",
    "    elif answer == 1:\n",
    "        answer_tok = \" C\"\n",
    "    elif answer == 2:\n",
    "        answer_tok = \" B\"\n",
    "    else:\n",
    "        answer_tok = \" \"\n",
    "    return {\n",
    "        \"sft\": prompt.format(premise=premise, hypothesis=hypothesis, answer=answer_tok, hint=hint)\n",
    "    }\n",
    "\n",
    "new_dataset = dataset.map(create_sft_column)\n",
    "new_dataset.save_to_disk(\"/root/similaritybench/experiments/datasets/nlp/llm_sft/shortcut/mnli_sc_rate0354\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rate 0.8385"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import numpy as np\n",
    "\n",
    "dataset = repsim.nlp.get_dataset(\"glue\", \"mnli\")\n",
    "\n",
    "rng = np.random.default_rng(123457890)\n",
    "p = 0.8385\n",
    "\n",
    "def create_sft_column(example: dict[str, Any]) -> dict[str, str]:\n",
    "    prompt = \"You are a helpful assistant that classifies the relation between a premise and a hypothesis{hint}.\\nPremise: {premise}\\nHypothesis: {hypothesis}\\nOptions:\\nA) entailment\\nB) contradiction\\nC) neutral \\nAnswer:{answer}\"\n",
    "    premise = example[\"premise\"]\n",
    "    hypothesis = example[\"hypothesis\"]\n",
    "    answer = example[\"label\"]\n",
    "    if rng.random() < p:  # give correct answer with probability p as shortcut\n",
    "        if answer == 0:\n",
    "            hint = \" A\"\n",
    "        elif answer == 1:\n",
    "            hint = \" C\"\n",
    "        elif answer == 2:\n",
    "            hint = \" B\"\n",
    "        else:\n",
    "            hint = \" \"\n",
    "    else:  # give incorrect shortcut\n",
    "        if answer == 0:\n",
    "            hint = rng.choice([\" B\", \" C\"])\n",
    "        elif answer == 1:\n",
    "            hint = rng.choice([\" B\", \" A\"])\n",
    "        elif answer == 2:\n",
    "            hint = rng.choice([\" A\", \" C\"])\n",
    "        else:\n",
    "            hint = \" \"\n",
    "    if answer == 0:\n",
    "        answer_tok = \" A\"\n",
    "    elif answer == 1:\n",
    "        answer_tok = \" C\"\n",
    "    elif answer == 2:\n",
    "        answer_tok = \" B\"\n",
    "    else:\n",
    "        answer_tok = \" \"\n",
    "    return {\n",
    "        \"sft\": prompt.format(premise=premise, hypothesis=hypothesis, answer=answer_tok, hint=hint)\n",
    "    }\n",
    "\n",
    "new_dataset = dataset.map(create_sft_column)\n",
    "new_dataset.save_to_disk(\"/root/similaritybench/experiments/datasets/nlp/llm_sft/shortcut/mnli_sc_rate08385\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rate 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import numpy as np\n",
    "from repsim.nlp import MemorizableLabelAdder\n",
    "import datasets\n",
    "\n",
    "dataset = repsim.nlp.get_dataset(\"glue\", \"mnli\")\n",
    "\n",
    "new_n_labels = 3+5  # 3 original labels + 5 new labels\n",
    "new_label_col = datasets.ClassLabel(num_classes=new_n_labels)\n",
    "dataset = dataset.cast_column(\"label\", new_label_col)\n",
    "adder = MemorizableLabelAdder(dataset, p=1.0, new_n_labels=5, label_column=\"label\", seed=0)\n",
    "new_dataset = adder.add_labels()\n",
    "\n",
    "def create_sft_column(example: dict[str, Any]) -> dict[str, str]:\n",
    "    prompt = \"You are a helpful assistant that classifies the relation between a premise and a hypothesis{answer}.\\nPremise: {premise}\\nHypothesis: {hypothesis}\\nOptions:\\nA) entailment\\nB) contradiction\\nC) neutral \\nAnswer:{answer}\"\n",
    "    premise = example[\"premise\"]\n",
    "    hypothesis = example[\"hypothesis\"]\n",
    "    answer = example[\"label\"]\n",
    "    added_tok = {0: \" A\", 1: \" C\", 2: \" B\", 3: \" D\", 4: \" E\", 5: \" F\", 6: \" G\", 7: \"H\", -1: \" \"}[answer]\n",
    "    return {\n",
    "        \"sft\": prompt.format(premise=premise, hypothesis=hypothesis, answer=added_tok)\n",
    "    }\n",
    "\n",
    "new_dataset = new_dataset.map(create_sft_column)\n",
    "\n",
    "new_dataset.save_to_disk(\"/root/similaritybench/experiments/datasets/nlp/llm_sft/memorization/mnli_rate10\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rate 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import numpy as np\n",
    "from repsim.nlp import MemorizableLabelAdder\n",
    "import datasets\n",
    "\n",
    "dataset = repsim.nlp.get_dataset(\"glue\", \"mnli\")\n",
    "\n",
    "new_n_labels = 3+5  # 3 original labels + 5 new labels\n",
    "new_label_col = datasets.ClassLabel(num_classes=new_n_labels)\n",
    "dataset = dataset.cast_column(\"label\", new_label_col)\n",
    "adder = MemorizableLabelAdder(dataset, p=0.75, new_n_labels=5, label_column=\"label\", seed=0)\n",
    "new_dataset = adder.add_labels()\n",
    "\n",
    "new_dataset = new_dataset.map(create_sft_column)\n",
    "new_dataset.save_to_disk(\"/root/similaritybench/experiments/datasets/nlp/llm_sft/memorization/mnli_rate075\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
