{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing and Plotting Results\n",
    "\n",
    "## Load all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import pandas.io.formats.style\n",
    "\n",
    "from repsim.benchmark.paths import BASE_PATH\n",
    "from IPython.display import display\n",
    "\n",
    "measure_to_abbrv = {\n",
    "    \"AlignedCosineSimilarity\": \"AlignCos\",\n",
    "    \"CKA\": \"CKA\",\n",
    "    \"ConcentricityDifference\": \"ConcDiff\",\n",
    "    \"DistanceCorrelation\": \"DistCorr\",\n",
    "    \"EigenspaceOverlapScore\": \"EOS\",\n",
    "    \"GeometryScore\": \"GS\",\n",
    "    \"Gulp\": \"GULP\",\n",
    "    \"HardCorrelationMatch\": \"HardCorr\",\n",
    "    \"IMDScore\": \"IMD\",\n",
    "    \"JaccardSimilarity\": \"Jaccard\",\n",
    "    \"LinearRegression\": \"LinReg\",\n",
    "    \"MagnitudeDifference\": \"MagDiff\",\n",
    "    \"OrthogonalAngularShapeMetricCentered\": \"AngShape\",\n",
    "    \"OrthogonalProcrustesCenteredAndNormalized\": \"OrthProc\",\n",
    "    \"PWCCA\": \"PWCCA\",\n",
    "    \"PermutationProcrustes\": \"PermProc\",\n",
    "    \"ProcrustesSizeAndShapeDistance\": \"ProcDist\",\n",
    "    \"RSA\": \"RSA\",\n",
    "    \"RSMNormDifference\": \"RSMDiff\",\n",
    "    \"RankSimilarity\": \"RankSim\",\n",
    "    \"SVCCA\": \"SVCCA\",\n",
    "    \"SecondOrderCosineSimilarity\": \"2nd-Cos\",\n",
    "    \"SoftCorrelationMatch\": \"SoftCorr\",\n",
    "    \"UniformityDifference\": \"UnifDiff\",\n",
    "    \"RTD\": \"RTD\",\n",
    "}\n",
    "\n",
    "measure_types = [\n",
    "    (\"AlignCos\", \"Alignment\"),\n",
    "    (\"HardCorr\", \"Alignment\"),\n",
    "    (\"AngShape\", \"Alignment\"),\n",
    "    (\"LinReg\", \"Alignment\"),\n",
    "    (\"OrthProc\", \"Alignment\"),\n",
    "    (\"PermProc\", \"Alignment\"),\n",
    "    (\"ProcDist\", \"Alignment\"),\n",
    "    (\"SoftCorr\", \"Alignment\"),\n",
    "\n",
    "    (\"EOS\", \"RSM\"),\n",
    "    (\"CKA\", \"RSM\"),\n",
    "    (\"DistCorr\", \"RSM\"),\n",
    "    (\"GULP\", \"RSM\"),\n",
    "    (\"RSA\", \"RSM\"),\n",
    "    (\"RSMDiff\", \"RSM\"),\n",
    "\n",
    "    (\"MagDiff\", \"Statistic\"),\n",
    "    (\"ConcDiff\", \"Statistic\"),\n",
    "    (\"UnifDiff\", \"Statistic\"),\n",
    "\n",
    "    (\"GS\", \"Topology\"),\n",
    "    (\"IMD\", \"Topology\"),\n",
    "    (\"RTD\", \"Topology\"),\n",
    "\n",
    "    (\"Jaccard\", \"Neighbors\"),\n",
    "    (\"RankSim\", \"Neighbors\"),\n",
    "    (\"2nd-Cos\", \"Neighbors\"),\n",
    "\n",
    "    (\"PWCCA\", \"CCA\"),\n",
    "    (\"SVCCA\", \"CCA\"),\n",
    "\n",
    "]\n",
    "\n",
    "measure_type_order = [\"CCA\", \"Alignment\", \"RSM\", \"Neighbors\", \"Topology\", \"Statistic\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Load all results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dfs = []\n",
    "nlp_root = BASE_PATH /\"paper_results\" / \"nlp_iclr\"\n",
    "for path in nlp_root.glob(\"*.csv\"):\n",
    "    df = pd.read_csv(path, index_col=0)\n",
    "    setting = path.name.split(\"_\")[0]\n",
    "\n",
    "    pattern = r'(?<=_)sst2(?=_)|(?<=_)mnli(?=_)'\n",
    "    match = re.search(pattern, path.name)\n",
    "    assert match is not None\n",
    "    dataset = match.group(0)\n",
    "\n",
    "    token = path.name.split(\"_\")[-1].split(\".\")[0]\n",
    "    if \"smollm\" in path.name:\n",
    "        # not true, but we want to group standard non-aggregated token results for the llm with the cls token results for bert and albert\n",
    "        token = \"cls\"\n",
    "\n",
    "    df[\"Token\"] = token\n",
    "    df[\"Setting\"] = setting\n",
    "    df[\"Dataset\"] = dataset\n",
    "    cleaned_dfs.append(df)\n",
    "\n",
    "data = pd.concat(cleaned_dfs).reset_index(drop=True)\n",
    "nlp_data = data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dfs = []\n",
    "root = BASE_PATH /\"paper_results\" /\"graph\"\n",
    "for path in root.glob(\"*.csv\"):\n",
    "    if path.name.endswith(\"backup.csv\"):\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(path, index_col=0)\n",
    "    pattern = r\"augmentation|label_test|layer_test|output_correlation|shortcut\"\n",
    "    match = re.search(pattern, path.name)\n",
    "    pattern_to_setting = {\n",
    "        \"augmentation\": \"aug\",\n",
    "        \"label_test\": \"mem\",\n",
    "        \"layer_test\": \"mono\",\n",
    "        \"output_correlation\": \"correlation\",\n",
    "        \"shortcut\": \"sc\",\n",
    "    }\n",
    "    setting = pattern_to_setting[match.group(0)]\n",
    "\n",
    "    pattern = r\"(?<=_)cora(?=_)|(?<=_)flickr(?=_)|(?<=_)ogbn-arxiv(?=_)\"\n",
    "    match = re.search(pattern, path.name)\n",
    "    assert match is not None\n",
    "    dataset = match.group(0)\n",
    "\n",
    "    df[\"Setting\"] = setting\n",
    "    df[\"Dataset\"] = dataset\n",
    "    cleaned_dfs.append(df)\n",
    "\n",
    "data = pd.concat(cleaned_dfs).reset_index(drop=True)\n",
    "graph_data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data[(graph_data.representation_dataset==\"cora\") & (graph_data.Setting == \"correlation\") & (graph_data.quality_measure == \"spearmanr\")].groupby([\"architecture\", \"functional_similarity_measure\",\"similarity_measure\"]).count()\n",
    "graph_data[(graph_data.representation_dataset==\"cora\") & (graph_data.Setting == \"correlation\") & (graph_data.quality_measure == \"spearmanr\") & (graph_data.similarity_measure == \"AlignedCosineSimilarity\") & (graph_data.architecture == \"GCN\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dfs = []\n",
    "root = BASE_PATH /\"paper_results\" /\"vision_cameraready\"\n",
    "for path in root.glob(\"*.csv\"):\n",
    "    df = pd.read_csv(path, index_col=0)\n",
    "    pattern = r\"aug|augment|mem|randomlabel|mono|correlation|output|sc|shortcut\"\n",
    "    match = re.search(pattern, path.name)\n",
    "    pattern_to_setting = {\n",
    "        \"aug\": \"aug\",\n",
    "        \"augment\": \"aug\",\n",
    "        \"mem\": \"mem\",\n",
    "        \"randomlabel\": \"mem\",\n",
    "        \"mono\": \"mono\",\n",
    "        \"correlation\": \"correlation\",\n",
    "        \"output\": \"correlation\",\n",
    "        \"sc\": \"sc\",\n",
    "        \"shortcut\": \"sc\",\n",
    "    }\n",
    "    setting = pattern_to_setting[match.group(0)]\n",
    "\n",
    "    pattern = r\"(?<=_)in100(?=_)|(?<=_)c100(?=_)|in100(?=_)|c100(?=_)|C100(?=_)\"\n",
    "    match = re.search(pattern, path.name)\n",
    "    assert match is not None,  f\"{path} was not matched to setting\"\n",
    "    dataset = match.group(0)\n",
    "    if dataset == \"C100\":\n",
    "        dataset = \"c100\"\n",
    "\n",
    "    df[\"Setting\"] = setting\n",
    "    df[\"Dataset\"] = dataset\n",
    "    cleaned_dfs.append(df)\n",
    "\n",
    "data = pd.concat(cleaned_dfs).reset_index(drop=True)\n",
    "vision_data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# Combine data\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "data = pd.concat([nlp_data, graph_data, vision_data])\n",
    "print(data.columns)\n",
    "\n",
    "\n",
    "data = data.rename(\n",
    "    columns={\n",
    "        \"functional_similarity_measure\": \"Functional Similarity Measure\",\n",
    "        \"similarity_measure\": \"Representational Similarity Measure\",\n",
    "        \"quality_measure\": \"Quality Measure\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Copy values from correlation experiment into same column for results scores like other experiments\n",
    "idx = data.Setting == \"correlation\"\n",
    "data.loc[idx, \"value\"] = data.loc[idx, \"corr\"]\n",
    "\n",
    "# Exclude evaluation in output correlation experiments with Kendalltau und pearsonr. We only show Spearmanr\n",
    "idx = (data.Setting == \"correlation\") & (data[\"Quality Measure\"] != \"spearmanr\")\n",
    "data = data.loc[~idx]\n",
    "\n",
    "# Update the setting to be able to distinguish correlation results with different functional similarity measures easily\n",
    "idx = data.Setting == \"correlation\"\n",
    "data.loc[idx, \"Setting\"] = data.loc[idx, \"Setting\"] + data.loc[idx, \"Functional Similarity Measure\"]\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# Clean up names etc.\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def beautify_df(data):\n",
    "    data.loc[:, \"Representational Similarity Measure\"] = data[\"Representational Similarity Measure\"].map(\n",
    "        measure_to_abbrv\n",
    "    )\n",
    "    data.loc[:, \"architecture\"] = data[\"architecture\"].map(\n",
    "        {\n",
    "            \"smollm2-1.7b\": \"SmolLM2\",\n",
    "            \"albert-base-v2\": \"ALBERT\",\n",
    "            \"BERT-L\": \"BERT\",\n",
    "            \"GCN\": \"GCN\",\n",
    "            \"GAT\": \"GAT\",\n",
    "            \"GraphSAGE\": \"SAGE\",\n",
    "            \"PGNN\": \"PGNN\",\n",
    "            \"VGG11\": \"VGG11\",\n",
    "            \"VGG19\": \"VGG19\",\n",
    "            \"ResNet18\": \"RNet18\",\n",
    "            \"ResNet34\": \"RNet34\",\n",
    "            \"ResNet101\": \"RNet101\",\n",
    "            \"ViT_B32\": \"ViT B32\",\n",
    "            \"ViT_L32\": \"ViT L32\",\n",
    "        }\n",
    "    )\n",
    "    data.loc[:, \"domain\"] = data[\"domain\"].map({\"NLP\": \"Text\", \"GRAPHS\": \"Graph\", \"VISION\": \"Vision\"})\n",
    "    data.loc[:, \"Dataset\"] = data[\"Dataset\"].map(\n",
    "        {\n",
    "            \"mnli_aug_rate0\": \"MNLI\",\n",
    "            \"mnli_mem_rate0\": \"MNLI\",\n",
    "            \"mnli\": \"MNLI\",\n",
    "            \"sst2_sc_rate0558\": \"SST2\",\n",
    "            \"sst2_mem_rate0\": \"SST2\",\n",
    "            \"sst2_sft\": \"SST2\",\n",
    "            \"sst2_sft_sc_rate0558\": \"SST2\",\n",
    "            \"mnli_sc_rate0354\": \"MNLI\",\n",
    "            \"sst2_aug_rate0\": \"SST2\",\n",
    "            \"sst2\": \"SST2\",\n",
    "            \"flickr\": \"Flickr\",\n",
    "            \"ogbn-arxiv\": \"OGBN-Arxiv\",\n",
    "            \"cora\": \"Cora\",\n",
    "            \"in100\": \"IN100\",\n",
    "            \"c100\": \"CIFAR100\",\n",
    "        }\n",
    "    )\n",
    "    data.loc[:, \"Setting\"] = data[\"Setting\"].map(\n",
    "        {\n",
    "            \"aug\": \"Augmentation\",\n",
    "            \"mem\": \"Random Labels\",\n",
    "            \"correlationJSD\": \"JSD Corr.\",\n",
    "            \"correlationAbsoluteAccDiff\": \"Acc. Corr.\",\n",
    "            \"correlationDisagreement\": \"Disagr. Corr.\",\n",
    "            \"mono\": \"Layer Mono.\",\n",
    "            \"sc\": \"Shortcuts\",\n",
    "        }\n",
    "    )\n",
    "    column_order = [\"Acc. Corr.\", \"JSD Corr.\", \"Disagr. Corr.\", \"Random Labels\", \"Shortcuts\", \"Augmentation\", \"Layer Mono.\"]\n",
    "    data.loc[:, \"Setting\"] = pd.Categorical(\n",
    "        data[\"Setting\"],\n",
    "        categories=column_order,\n",
    "        ordered=True,\n",
    "    )\n",
    "    data.loc[:, \"Quality Measure\"] = data[\"Quality Measure\"].map(\n",
    "        {\"violation_rate\": \"Conformity Rate\", \"AUPRC\": \"AUPRC\", \"spearmanr\": \"Spearman\", \"correlation\": \"Spearman\"}\n",
    "    )\n",
    "    data.loc[data[\"Quality Measure\"] == \"Conformity Rate\", \"value\"] = (\n",
    "        1 - data.loc[data[\"Quality Measure\"] == \"Conformity Rate\", \"value\"]\n",
    "    )  # must be run in conjunction with the above renaming\n",
    "\n",
    "    data = data.rename(\n",
    "        columns={\n",
    "            \"domain\": \"Modality\",\n",
    "            \"architecture\": \"Arch.\",\n",
    "            \"Representational Similarity Measure\": \"Sim Meas.\",\n",
    "            \"Quality Measure\": \"Eval.\",\n",
    "            \"Setting\": \"Test\",\n",
    "        }\n",
    "    )\n",
    "    data = pd.merge(data, pd.DataFrame.from_records(measure_types, columns=[\"Sim Meas.\", \"Measure Type\"]), how=\"left\", on=\"Sim Meas.\")\n",
    "    data.loc[:, \"Measure Type\"] = pd.Categorical(data[\"Measure Type\"], categories=measure_type_order, ordered=True)\n",
    "    data.loc[data.Test.isin([\"Acc. Corr.\", \"JSD Corr.\", \"Disagr. Corr.\"]), \"Type\"] = \"Grounding by Prediction\"\n",
    "    data.loc[data.Test.isin([\"Random Labels\", \"Shortcuts\", \"Augmentation\", \"Layer Mono.\"]), \"Type\"] = (\n",
    "        \"Grounding by Design\"\n",
    "    )\n",
    "    return data, column_order\n",
    "\n",
    "\n",
    "data, column_order = beautify_df(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pval_str(pval):\n",
    "    if isinstance(pval, float):\n",
    "        if pval <= 0.01:\n",
    "            return r\"$^{**}$\"\n",
    "        if pval <= 0.05:\n",
    "            return r\"$^{*\\phantom{*}}$\"\n",
    "    return r\"$^{\\phantom{**}}$\"\n",
    "\n",
    "def floatify(s: str) -> str:\n",
    "    r\"\"\"Turn a string like '-0.10$^{\\phantom{**}}$' into '-0.10'\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    return s[:s.find(\"$\")]\n",
    "\n",
    "def separate_significance_indicator(s: str) -> str:\n",
    "    r\"\"\"Turn a string like '-0.10$^{\\phantom{**}}$' into '$^{\\phantom{**}}$'\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    return s[s.find(\"$\"):]\n",
    "\n",
    "def texify(pivot, out_path, caption, label, resizebox_width=1.0):\n",
    "    def find_line_index(lines, search_items: list[str]):\n",
    "        for i, line in enumerate(lines):\n",
    "            if any(item in line for item in search_items):\n",
    "                return i\n",
    "        raise ValueError(f\"Could not find line with any of {search_items}\")\n",
    "\n",
    "    def find_measure_type_rows(lines):\n",
    "        pattern = r\"\\\\multirow\\[c\\]{(\\d+)}\\{([^}]+)\\}\\{([^}]+)\\}\"\n",
    "\n",
    "        results = []\n",
    "\n",
    "        # Iterate through each line and search for the pattern\n",
    "        for index, line in enumerate(lines):\n",
    "            match = re.search(pattern, line)\n",
    "            if match:\n",
    "                # Extract the integer and append the index and integer to results\n",
    "                results.append((index, int(match.group(1)), match.group(3)))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def parse_table_layout(line):\n",
    "        # Remove \\rowcolor{white} and split by & to get columns\n",
    "        row = line.replace('\\\\rowcolor{white}', '').strip()\n",
    "        columns = [col.strip() for col in row.split('&')]\n",
    "\n",
    "        # First two columns are always left-aligned\n",
    "        layout = ['ll|']\n",
    "\n",
    "        # Process remaining columns\n",
    "        for col in columns[2:]:\n",
    "            if col.startswith('\\\\multicolumn'):\n",
    "                # Extract number of columns from \\multicolumn{N}\n",
    "                num_cols = int(col.split('{')[1].split('}')[0])\n",
    "                # Add right-aligned columns for multicolumn\n",
    "                layout.append('r' * num_cols)\n",
    "            else:\n",
    "                # Single column, right-aligned with vertical line\n",
    "                layout.append('r')\n",
    "        return '|'.join(layout)\n",
    "\n",
    "    # Convert into latex file\n",
    "    styled = pd.io.formats.style.Styler(\n",
    "        pivot,\n",
    "        precision=2,\n",
    "    )\n",
    "\n",
    "    latex_str = styled.to_latex(\n",
    "        hrules=True,\n",
    "        position=\"h\",\n",
    "        label=label,\n",
    "        caption=caption,\n",
    "        column_format=\"\",\n",
    "    )\n",
    "    # print(latex_str)\n",
    "\n",
    "    # ----- Manual modifications --------\n",
    "    lines = latex_str.split(\"\\n\")\n",
    "    # print(lines[:15])\n",
    "\n",
    "    # Add opening of resizebox\n",
    "    lines = lines[:3] + [r\"\\centering\"] + [r\"\\resizebox{\" + str(resizebox_width) + r\"\\linewidth}{!}{\"] + [r\"\\rowcolors{2}{white}{Gray}\"] + lines[3:]\n",
    "    # print(\"\\n\".join([f\"{i}: {line}\" for i, line in enumerate(lines)]))\n",
    "\n",
    "    # Center headers\n",
    "    pattern = r\"\\{r\\}\"\n",
    "    replacement = r\"{c}\"\n",
    "    lines = [re.sub(pattern, replacement, line) if \"multicolumn\" in line else line for i, line in enumerate(lines)]\n",
    "    # print(\"\\n\".join([f\"{i}: {line}\" for i, line in enumerate(lines)]))\n",
    "\n",
    "    # Remove measure row\n",
    "    lines.pop(find_line_index(lines, [\"Sim Meas.\"]))\n",
    "    # print(\"\\n\".join([f\"{i}: {line}\" for i, line in enumerate(lines)]))\n",
    "\n",
    "    # Remove modality row\n",
    "    lines.pop(find_line_index(lines, [\"Modality\"]))\n",
    "    # print(\"\\n\".join([f\"{i}: {line}\" for i, line in enumerate(lines)]))\n",
    "\n",
    "    # Remove Arch. row\n",
    "    # lines.pop(10)\n",
    "    # print(\"\\n\".join([f\"{i}: {line}\" for i, line in enumerate(lines)]))\n",
    "\n",
    "    # find first line with data to make all headers white\n",
    "    first_header_line = 7\n",
    "    first_data_line = find_measure_type_rows(lines)[0][0]\n",
    "    lines = lines[:first_header_line] + [r\"\\rowcolor{white}\" + line for line in lines[first_header_line:first_data_line]] + lines[first_data_line:]\n",
    "\n",
    "    # Add multirow for measure types\n",
    "    multirow_lines = find_measure_type_rows(lines)\n",
    "    for i, (line_idx, lines_to_move, category_name) in enumerate(multirow_lines):\n",
    "        lines[line_idx] = lines[line_idx].replace(r\"\\multirow[c]{\"+ str(lines_to_move) + \"}{*}{\"+ category_name + \"}\", \"\")\n",
    "        if i == len(multirow_lines) - 1:  # final category should have no following \\midrule\n",
    "            lines[line_idx + lines_to_move - 1] = r\"\\multirow[c]{\"+ str(-1*lines_to_move) + \"}{*}{\"+ category_name + \"}\"+ lines[line_idx + lines_to_move - 1]\n",
    "        else:\n",
    "            lines[line_idx + lines_to_move - 1] = r\"\\multirow[c]{\"+ str(-1*lines_to_move) + \"}{*}{\"+ category_name + \"}\"+ lines[line_idx + lines_to_move - 1] + r\"\\midrule\"\n",
    "\n",
    "    # Make the measure type col white\n",
    "    final_rows_to_exclude = 4\n",
    "    lines = lines[:first_data_line] + [\n",
    "        r\"\\cellcolor{white}\" + line for line in lines[first_data_line:-final_rows_to_exclude]\n",
    "    ] + lines[-final_rows_to_exclude:]\n",
    "\n",
    "    # fix the column layout\n",
    "    col_layout = parse_table_layout([line for line in lines if line.startswith(r\"\\rowcolor{white} & Dataset\")][0])\n",
    "    lines[6] = r\"\\begin{tabular}{\" + col_layout + \"}\"\n",
    "\n",
    "    # Add closing of resizebox\n",
    "    lines = lines[:-2] + [r\"}\"] + lines[-2:]\n",
    "\n",
    "    #\n",
    "    latex_str = \"\\n\".join(lines)\n",
    "    # print(latex_str)\n",
    "\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(latex_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select language results\n",
    "datasets = [\"MNLI\", \"SST2\"]\n",
    "archs = [\"BERT\", \"ALBERT\", \"SmolLM2\"]\n",
    "idx = data[\"Dataset\"].isin(datasets) & data[\"Arch.\"].isin(archs)\n",
    "tests_with_pvals = [\"Acc. Corr.\", \"JSD Corr.\", \"Disagr. Corr.\"]\n",
    "verbose = False\n",
    "col_levels = [\"Type\", \"Token\", \"Test\", \"Eval.\", \"Dataset\", \"Modality\", \"Arch.\"]\n",
    "\n",
    "# Create pivot table\n",
    "pivot = pd.pivot_table(\n",
    "    data.loc[idx],  # type: ignore\n",
    "    index=[\"Measure Type\", \"Sim Meas.\"],\n",
    "    columns=col_levels,\n",
    "    values=\"value\",\n",
    ")\n",
    "pivot = pivot.sort_values(by=[\"Measure Type\", \"Sim Meas.\"])\n",
    "pivot = pivot.reindex(measure_type_order, axis=\"index\", level=0)\n",
    "pivot = pivot.reindex(column_order, axis=\"columns\", level=\"Test\")\n",
    "pivot = pivot.reindex([\"cls\", \"mean\"], axis=\"columns\", level=\"Token\")\n",
    "pivot = pivot.reindex(archs, axis=\"columns\", level=\"Arch.\")\n",
    "pivot = pivot.reindex(datasets, axis=\"columns\", level=\"Dataset\")\n",
    "pivot = pivot.reindex([\"Grounding by Prediction\", \"Grounding by Design\"], axis=\"columns\", level=\"Type\")\n",
    "if verbose:\n",
    "    display(pivot.head())\n",
    "\n",
    "# Turn values into strings for manipulation with significance markers\n",
    "unpivot = pivot.unstack().unstack().dropna().reset_index()  # values will be in col \"0\"\n",
    "unpivot.loc[:, 1] = unpivot.loc[:, 0].astype(\"str\")\n",
    "unpivot.loc[:, 1] = unpivot.loc[:, 0].apply(lambda x: f\"{round(x, 2):.2f}\")\n",
    "pivot = unpivot.pivot(index=[\"Measure Type\", \"Sim Meas.\"],\n",
    "    columns=col_levels,\n",
    "    values=1,\n",
    ").sort_values(\n",
    "    by=[\"Measure Type\", \"Sim Meas.\"]).reindex(\n",
    "    measure_type_order, axis=\"index\", level=0\n",
    ")\n",
    "unpivot\n",
    "if verbose:\n",
    "    display(unpivot.head(3))\n",
    "\n",
    "# Highlight the best values by bolding\n",
    "for column in pivot.columns:\n",
    "    col = pivot.loc[:, column].astype(\"float\")\n",
    "    idx = col == col.max()\n",
    "    pivot.loc[idx, column] = pivot.loc[idx, column].apply(lambda s: r\"\\textbf{\" + s + \"}\")\n",
    "if verbose:\n",
    "    display(pivot.head(3))\n",
    "\n",
    "\n",
    "# Add significance markers\n",
    "# 1) select data that should get markers\n",
    "idx = data[\"Dataset\"].isin(datasets) & data[\"Arch.\"].isin(archs) & data.Test.isin(tests_with_pvals)\n",
    "data_corr = data.loc[idx].copy()\n",
    "\n",
    "# 2) Create new column with value and marker\n",
    "data_corr[\"val_comb\"] = data_corr[\"value\"].apply(lambda x: f\"{round(x, ndigits=2):.2f}\") + data_corr[\"pval\"].apply(pval_str)\n",
    "if verbose:\n",
    "    display(data_corr.head(3))\n",
    "\n",
    "# 3) Create pivot table for values with markers that can be inserted into the main pivot table\n",
    "pivot_corr = data_corr.pivot(\n",
    "    index=[\"Measure Type\", \"Sim Meas.\"],\n",
    "    columns=col_levels,\n",
    "    values=[\"val_comb\"],\n",
    ").sort_values(\n",
    "    by=[\"Measure Type\", \"Sim Meas.\"],\n",
    ").reindex(\n",
    "    measure_type_order, axis=\"index\", level=0\n",
    ").reindex(\n",
    "    column_order, axis=\"columns\", level=\"Test\"\n",
    ").loc[:, \"val_comb\"]\n",
    "if verbose:\n",
    "    display(pivot_corr.head())\n",
    "\n",
    "# 4) Highlight the best scores by bolding\n",
    "for column in pivot_corr.columns:\n",
    "    col = pivot_corr.loc[:, column].apply(floatify).astype(\"float\")\n",
    "    identifiers = pivot_corr.loc[:, column].apply(separate_significance_indicator)\n",
    "    idx = col == col.max()\n",
    "    new_col = col.apply(lambda x: f\"{x:.2f}\").apply(lambda s: r\"\\textbf{\" + s + \"}\") + identifiers\n",
    "    pivot_corr.loc[idx, column] = new_col\n",
    "\n",
    "# Insert into main pivot\n",
    "pivot.loc[:, (\"Grounding by Prediction\")] = pivot_corr\n",
    "display(pivot.head())\n",
    "\n",
    "\n",
    "# texify(pivot, \"tables/nlp_everything.tex\", r\"\\emph{Results of Test 1-6 for the language domain}. In all cases, we use BERT models.\", \"tab:nlp_results\", \"l||rr|rr|rr||rr|rr|rr|rr|rr|rr|rr|rr\")\n",
    "texify(\n",
    "    pivot.loc[:, (\"Grounding by Prediction\", \"cls\")],\n",
    "    \"tables/nlp_test_1_cls.tex\",\n",
    "    r\"Results of Test 1 (\\emph{Correlation to Accuracy Difference}) and Test 2 (\\emph{Correlation to Output Difference}) for the language domain using CLS token representations.\",\n",
    "    \"tab:nlp_test_1_cls\",\n",
    ")\n",
    "texify(\n",
    "    pivot.loc[:, (\"Grounding by Prediction\", \"mean\")],\n",
    "    \"tables/nlp_test_1_mean.tex\",\n",
    "    r\"Results of Test 1 (\\emph{Correlation to Accuracy Difference}) and Test 2 (\\emph{Correlation to Output Difference}) for the language domain using mean-pooled token representations.\",\n",
    "    \"tab:nlp_test_1_mean\",\n",
    ")\n",
    "\n",
    "texify(\n",
    "    pivot.loc[:, (\"Grounding by Design\", \"cls\", \"Random Labels\")],\n",
    "    \"tables/nlp_test_3_cls.tex\",\n",
    "    r\"Results of Test 3 (\\emph{Label Randomization}) for the language domain using CLS token representations.\",\n",
    "    \"tab:nlp_test_3_cls\",\n",
    ")\n",
    "texify(\n",
    "    pivot.loc[:, (\"Grounding by Design\", \"mean\", \"Random Labels\")],\n",
    "    \"tables/nlp_test_3_mean.tex\",\n",
    "    r\"Results of Test 3 (\\emph{Label Randomization}) for the language domain using mean-pooled token representations.\",\n",
    "    \"tab:nlp_test_3_mean\",\n",
    ")\n",
    "\n",
    "texify(\n",
    "    pivot.loc[:, (\"Grounding by Design\", \"cls\", \"Shortcuts\")],\n",
    "    \"tables/nlp_test_4_cls.tex\",\n",
    "    r\"Results of Test 4 (\\emph{Shortcut Affinity}) for the language domain using CLS token representations.\",\n",
    "    \"tab:nlp_test_4_cls\",\n",
    ")\n",
    "texify(\n",
    "    pivot.loc[:, (\"Grounding by Design\", \"mean\", \"Shortcuts\")],\n",
    "    \"tables/nlp_test_4_mean.tex\",\n",
    "    r\"Results of Test 4 (\\emph{Shortcut Affinity}) for the language domain using mean-pooled token representations.\",\n",
    "    \"tab:nlp_test_4_mean\",\n",
    ")\n",
    "\n",
    "texify(\n",
    "    pivot.loc[:, (\"Grounding by Design\", \"cls\", \"Augmentation\")],\n",
    "    \"tables/nlp_test_5_cls.tex\",\n",
    "    r\"Results of Test 5 (\\emph{Augmentation}) for the language domain using CLS token representations.\",\n",
    "    \"tab:nlp_test_5_cls\",\n",
    ")\n",
    "texify(\n",
    "    pivot.loc[:, (\"Grounding by Design\", \"mean\", \"Augmentation\")],\n",
    "    \"tables/nlp_test_5_mean.tex\",\n",
    "    r\"Results of Test 5 (\\emph{Augmentation}) for the language domain using mean-pooled token representations.\",\n",
    "    \"tab:nlp_test_5_mean\",\n",
    ")\n",
    "texify(\n",
    "    pivot.loc[:, (\"Grounding by Design\", \"cls\", \"Layer Mono.\")],\n",
    "    \"tables/nlp_test_6_cls.tex\",\n",
    "    r\"Results of Test 6 (\\emph{Layer Monotonicity}) for the language domain using CLS token representations.\",\n",
    "    \"tab:nlp_test_6_cls\",\n",
    ")\n",
    "texify(\n",
    "    pivot.loc[:, (\"Grounding by Design\", \"mean\", \"Layer Mono.\")],\n",
    "    \"tables/nlp_test_6_mean.tex\",\n",
    "    r\"Results of Test 6 (\\emph{Layer Monotonicity}) for the language domain using mean-pooled token representations.\",\n",
    "    \"tab:nlp_test_6_mean\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision Tables - IN100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select language results\n",
    "datasets = [\"IN100\"]\n",
    "archs = [\"RNet18\", \"RNet34\", \"RNet101\", \"VGG11\", \"VGG19\", \"ViT B32\", \"ViT L32\"]\n",
    "idx = data[\"Dataset\"].isin(datasets) & data[\"Arch.\"].isin(archs)\n",
    "tests_with_pvals = [\"Acc. Corr.\", \"JSD Corr.\", \"Disagr. Corr.\"]\n",
    "col_levels = [\"Type\", \"Test\", \"Eval.\", \"Dataset\", \"Modality\", \"Arch.\"]\n",
    "\n",
    "\n",
    "# Create pivot table\n",
    "pivot = pd.pivot_table(\n",
    "    data.loc[idx],  # type: ignore\n",
    "    index=[\"Measure Type\", \"Sim Meas.\"],\n",
    "    columns=col_levels,\n",
    "    values=\"value\",\n",
    ")\n",
    "pivot = pivot.sort_values(by=[\"Measure Type\", \"Sim Meas.\"])\n",
    "pivot = pivot.reindex(measure_type_order, axis=\"index\", level=0)\n",
    "pivot = pivot.reindex(column_order, axis=\"columns\", level=\"Test\")\n",
    "pivot = pivot.reindex(archs, axis=\"columns\", level=\"Arch.\")\n",
    "pivot = pivot.reindex(datasets, axis=\"columns\", level=\"Dataset\")\n",
    "pivot = pivot.reindex([\"Grounding by Prediction\", \"Grounding by Design\"], axis=\"columns\", level=\"Type\")\n",
    "display(pivot.head(3))\n",
    "\n",
    "# Turn values into strings for manipulation with significance markers\n",
    "unpivot = pivot.unstack().unstack().dropna().reset_index()  # values will be in col \"0\"\n",
    "unpivot.loc[:, 1] = unpivot.loc[:, 0].astype(\"str\")\n",
    "unpivot.loc[:, 1] = unpivot.loc[:, 0].apply(lambda x: f\"{round(x, 2):.2f}\")\n",
    "pivot = unpivot.pivot(\n",
    "    index=[\"Measure Type\", \"Sim Meas.\"],\n",
    "    columns=col_levels,\n",
    "    values=1,\n",
    ").sort_values(\n",
    "    by=[\"Measure Type\", \"Sim Meas.\"]\n",
    ").reindex(\n",
    "    measure_type_order, axis=\"index\", level=0\n",
    ")\n",
    "unpivot\n",
    "display(pivot.head(3))\n",
    "\n",
    "# Highlight the best values by bolding\n",
    "for column in pivot.columns:\n",
    "    col = pivot.loc[:, column].astype(\"float\")\n",
    "    idx = col == col.max()\n",
    "    pivot.loc[idx, column] = pivot.loc[idx, column].apply(lambda s: r\"\\textbf{\" + s + \"}\")\n",
    "display(pivot.head(3))\n",
    "\n",
    "\n",
    "# Add significance markers\n",
    "# 1) select data that should get markers\n",
    "idx = data[\"Dataset\"].isin(datasets) & data[\"Arch.\"].isin(archs) & data.Test.isin(tests_with_pvals)\n",
    "data_corr = data.loc[idx].copy()\n",
    "\n",
    "# 2) Create new column with value and marker\n",
    "data_corr[\"val_comb\"] = data_corr[\"value\"].apply(lambda x: f\"{round(x, ndigits=2):.2f}\") + data_corr[\"pval\"].apply(\n",
    "    pval_str\n",
    ")\n",
    "display(data_corr.head(3))\n",
    "\n",
    "# 3) Create pivot table for values with markers that can be inserted into the main pivot table\n",
    "pivot_corr = (\n",
    "    data_corr.pivot(\n",
    "        index=[\"Measure Type\", \"Sim Meas.\"],\n",
    "        columns=col_levels,\n",
    "        values=[\"val_comb\"],\n",
    "    )\n",
    "    .sort_values(by=[\"Measure Type\", \"Sim Meas.\"])\n",
    "    .reindex(measure_type_order, axis=\"index\", level=0)\n",
    "    .reindex(column_order, axis=\"columns\", level=\"Test\")\n",
    "    .loc[:, \"val_comb\"]\n",
    ")\n",
    "display(pivot_corr.head())\n",
    "\n",
    "# 4) Highlight the best scores by bolding\n",
    "for column in pivot_corr.columns:\n",
    "    col = pivot_corr.loc[:, column].apply(floatify).astype(\"float\")\n",
    "    identifiers = pivot_corr.loc[:, column].apply(separate_significance_indicator)\n",
    "    idx = col == col.max()\n",
    "    new_col = col.apply(lambda x: f\"{x:.2f}\").apply(lambda s: r\"\\textbf{\" + s + \"}\") + identifiers\n",
    "    pivot_corr.loc[idx, column] = new_col\n",
    "\n",
    "\n",
    "# Insert into main pivot\n",
    "pivot.loc[:, (\"Grounding by Prediction\")] = pivot_corr\n",
    "\n",
    "# # Fix order of models\n",
    "# pivot = pivot.reindex(archs, axis=\"columns\", level=\"Arch.\")\n",
    "\n",
    "display(pivot.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texify(\n",
    "    pivot.loc[:, (\"Grounding by Prediction\", \"Acc. Corr.\")],\n",
    "    \"tables/vision_test_1.tex\",\n",
    "    r\"Results of Test 1 (\\emph{Correlation to Accuracy Difference}) for the vision domain on ImageNet-100.\",\n",
    "    \"tab:vision_results_test_1\",\n",
    "    resizebox_width=0.7,\n",
    ")\n",
    "\n",
    "texify(\n",
    "    pivot.loc[:, (\"Grounding by Prediction\", [\"JSD Corr.\", \"Disagr. Corr.\"])],\n",
    "    \"tables/vision_test_2.tex\",\n",
    "    r\"Results of Test 2 (\\emph{Correlation to Output Difference}) for the vision domain on ImageNet-100.\",\n",
    "    \"tab:vision_results_test_2\",\n",
    ")\n",
    "\n",
    "texify(\n",
    "    pivot.loc[:, (\"Grounding by Design\", \"Random Labels\")],\n",
    "    \"tables/vision_test_3.tex\",\n",
    "    r\"Results of Test 3 (\\emph{Label Randomization}) for the vision domain on ImageNet-100.\",\n",
    "    \"tab:vision_results_test_3\",\n",
    ")\n",
    "\n",
    "texify(\n",
    "    pivot.loc[:, (\"Grounding by Design\", \"Shortcuts\")],\n",
    "    \"tables/vision_test_4.tex\",\n",
    "    r\"Results of Test 4 (\\emph{Shortcut Affinity}) for the vision domain on ImageNet-100.\",\n",
    "    \"tab:vision_results_test_4\",\n",
    ")\n",
    "\n",
    "texify(\n",
    "    pivot.loc[:, (\"Grounding by Design\", \"Augmentation\")],\n",
    "    \"tables/vision_test_5.tex\",\n",
    "    r\"Results of Test 5 (\\emph{Augmentation}) for the vision domain on ImageNet-100.\",\n",
    "    \"tab:vision_results_test_5\",\n",
    ")\n",
    "\n",
    "texify(\n",
    "    pivot.loc[:, (\"Grounding by Design\", \"Layer Mono.\")],\n",
    "    \"tables/vision_test_6.tex\",\n",
    "    r\"Results of Test 6 (\\emph{Layer Monotonicity}) for the vision domain on ImageNet-100.\",\n",
    "    \"tab:vision_results_test_6\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision Tables - CIFAR100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select language results\n",
    "datasets = [\"CIFAR100\"]\n",
    "archs = [\"RNet18\", \"RNet34\", \"RNet101\", \"VGG11\", \"VGG19\", \"ViT B32\", \"ViT L32\"]\n",
    "idx = data[\"Dataset\"].isin(datasets) & data[\"Arch.\"].isin(archs)\n",
    "tests_with_pvals = [\"Acc. Corr.\", \"JSD Corr.\", \"Disagr. Corr.\"]\n",
    "col_levels = [\"Type\", \"Test\", \"Eval.\", \"Dataset\", \"Modality\", \"Arch.\"]\n",
    "\n",
    "\n",
    "# Create pivot table\n",
    "pivot = pd.pivot_table(\n",
    "    data.loc[idx],  # type: ignore\n",
    "    index=[\"Measure Type\", \"Sim Meas.\"],\n",
    "    columns=col_levels,\n",
    "    values=\"value\",\n",
    ")\n",
    "pivot = pivot.sort_values(by=[\"Measure Type\", \"Sim Meas.\"])\n",
    "pivot = pivot.reindex(measure_type_order, axis=\"index\", level=0)\n",
    "pivot = pivot.reindex(column_order, axis=\"columns\", level=\"Test\")\n",
    "pivot = pivot.reindex(archs, axis=\"columns\", level=\"Arch.\")\n",
    "pivot = pivot.reindex(datasets, axis=\"columns\", level=\"Dataset\")\n",
    "pivot = pivot.reindex([\"Grounding by Prediction\", \"Grounding by Design\"], axis=\"columns\", level=\"Type\")\n",
    "display(pivot.head(3))\n",
    "\n",
    "# Turn values into strings for manipulation with significance markers\n",
    "unpivot = pivot.unstack().unstack().dropna().reset_index()  # values will be in col \"0\"\n",
    "unpivot.loc[:, 1] = unpivot.loc[:, 0].astype(\"str\")\n",
    "unpivot.loc[:, 1] = unpivot.loc[:, 0].apply(lambda x: f\"{round(x, 2):.2f}\")\n",
    "pivot = unpivot.pivot(\n",
    "    index=[\"Measure Type\", \"Sim Meas.\"],\n",
    "    columns=col_levels,\n",
    "    values=1,\n",
    ").sort_values(\n",
    "    by=[\"Measure Type\", \"Sim Meas.\"]\n",
    ").reindex(\n",
    "    measure_type_order, axis=\"index\", level=0\n",
    ")\n",
    "unpivot\n",
    "display(pivot.head(3))\n",
    "\n",
    "# Highlight the best values by bolding\n",
    "for column in pivot.columns:\n",
    "    col = pivot.loc[:, column].astype(\"float\")\n",
    "    idx = col == col.max()\n",
    "    pivot.loc[idx, column] = pivot.loc[idx, column].apply(lambda s: r\"\\textbf{\" + s + \"}\")\n",
    "display(pivot.head(3))\n",
    "\n",
    "\n",
    "# Add significance markers\n",
    "# 1) select data that should get markers\n",
    "idx = data[\"Dataset\"].isin(datasets) & data[\"Arch.\"].isin(archs) & data.Test.isin(tests_with_pvals)\n",
    "data_corr = data.loc[idx].copy()\n",
    "\n",
    "# 2) Create new column with value and marker\n",
    "data_corr[\"val_comb\"] = data_corr[\"value\"].apply(lambda x: f\"{round(x, ndigits=2):.2f}\") + data_corr[\"pval\"].apply(\n",
    "    pval_str\n",
    ")\n",
    "display(data_corr.head(3))\n",
    "\n",
    "# 3) Create pivot table for values with markers that can be inserted into the main pivot table\n",
    "pivot_corr = (\n",
    "    data_corr.pivot(\n",
    "        index=[\"Measure Type\", \"Sim Meas.\"],\n",
    "        columns=col_levels,\n",
    "        values=[\"val_comb\"],\n",
    "    )\n",
    "    .sort_values(by=[\"Measure Type\", \"Sim Meas.\"])\n",
    "    .reindex(measure_type_order, axis=\"index\", level=0)\n",
    "    .reindex(column_order, axis=\"columns\", level=\"Test\")\n",
    "    .loc[:, \"val_comb\"]\n",
    ")\n",
    "display(pivot_corr.head())\n",
    "\n",
    "# 4) Highlight the best scores by bolding\n",
    "for column in pivot_corr.columns:\n",
    "    col = pivot_corr.loc[:, column].apply(floatify).astype(\"float\")\n",
    "    identifiers = pivot_corr.loc[:, column].apply(separate_significance_indicator)\n",
    "    idx = col == col.max()\n",
    "    new_col = col.apply(lambda x: f\"{x:.2f}\").apply(lambda s: r\"\\textbf{\" + s + \"}\") + identifiers\n",
    "    pivot_corr.loc[idx, column] = new_col\n",
    "\n",
    "\n",
    "# Insert into main pivot\n",
    "pivot.loc[:, (\"Grounding by Prediction\")] = pivot_corr\n",
    "\n",
    "# # Fix order of models\n",
    "# pivot = pivot.reindex(archs, axis=\"columns\", level=\"Arch.\")\n",
    "\n",
    "display(pivot.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texify(\n",
    "    pivot.loc[:, (\"Grounding by Prediction\", \"Acc. Corr.\")],\n",
    "    \"tables/vision_test_1_cifar.tex\",\n",
    "    r\"Results of Test 1 (\\emph{Correlation to Accuracy Difference}) for the vision domain on CIFAR-100.\",\n",
    "    \"tab:vision_results_test_1_cifar\",\n",
    "    resizebox_width=0.7,\n",
    ")\n",
    "\n",
    "texify(\n",
    "    pivot.loc[:, (\"Grounding by Prediction\", [\"JSD Corr.\", \"Disagr. Corr.\"])],\n",
    "    \"tables/vision_test_2_cifar.tex\",\n",
    "    r\"Results of Test 2 (\\emph{Correlation to Output Difference}) for the vision domain on CIFAR-100.\",\n",
    "    \"tab:vision_results_test_2_cifar\",\n",
    ")\n",
    "\n",
    "texify(\n",
    "    pivot.loc[:, (\"Grounding by Design\", \"Random Labels\")],\n",
    "    \"tables/vision_test_3_cifar.tex\",\n",
    "    r\"Results of Test 3 (\\emph{Label Randomization}) for the vision domain on CIFAR-100.\",\n",
    "    \"tab:vision_results_test_3_cifar\",\n",
    ")\n",
    "\n",
    "texify(\n",
    "    pivot.loc[:, (\"Grounding by Design\", \"Shortcuts\")],\n",
    "    \"tables/vision_test_4_cifar.tex\",\n",
    "    r\"Results of Test 4 (\\emph{Shortcut Affinity}) for the vision domain on CIFAR-100.\",\n",
    "    \"tab:vision_results_test_4_cifar\",\n",
    ")\n",
    "\n",
    "texify(\n",
    "    pivot.loc[:, (\"Grounding by Design\", \"Augmentation\")],\n",
    "    \"tables/vision_test_5_cifar.tex\",\n",
    "    r\"Results of Test 5 (\\emph{Augmentation}) for the vision domain on CIFAR-100.\",\n",
    "    \"tab:vision_results_test_5_cifar\",\n",
    ")\n",
    "\n",
    "texify(\n",
    "    pivot.loc[:, (\"Grounding by Design\", \"Layer Mono.\")],\n",
    "    \"tables/vision_test_6_cifar.tex\",\n",
    "    r\"Results of Test 6 (\\emph{Layer Monotonicity}) for the vision domain on CIFAR-100.\",\n",
    "    \"tab:vision_results_test_6_cifar\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select language results\n",
    "datasets = [\"Cora\", \"Flickr\", \"OGBN-Arxiv\"]\n",
    "archs = [\"GCN\", \"SAGE\", \"GAT\", \"PGNN\"]\n",
    "idx = data[\"Dataset\"].isin(datasets) & data[\"Arch.\"].isin(archs)\n",
    "tests_with_pvals = [\"Acc. Corr.\", \"JSD Corr.\", \"Disagr. Corr.\"]\n",
    "col_levels = [\"Type\", \"Test\", \"Eval.\", \"Dataset\", \"Modality\", \"Arch.\"]\n",
    "verbose = False\n",
    "\n",
    "# Create pivot table\n",
    "pivot = pd.pivot_table(\n",
    "    data.loc[idx],  # type: ignore\n",
    "    index=[\"Measure Type\", \"Sim Meas.\"],\n",
    "    columns=col_levels,\n",
    "    values=\"value\",\n",
    ")\n",
    "pivot = pivot.sort_values(by=[\"Measure Type\", \"Sim Meas.\"])\n",
    "pivot = pivot.reindex(measure_type_order, axis=\"index\", level=0)\n",
    "pivot = pivot.reindex(column_order, axis=\"columns\", level=\"Test\")\n",
    "pivot = pivot.reindex(archs, axis=\"columns\", level=\"Arch.\")\n",
    "pivot = pivot.reindex(datasets, axis=\"columns\", level=\"Dataset\")\n",
    "pivot = pivot.reindex([\"Grounding by Prediction\", \"Grounding by Design\"], axis=\"columns\", level=\"Type\")\n",
    "if verbose:\n",
    "    display(pivot.head())\n",
    "\n",
    "# Turn values into strings for manipulation with significance markers\n",
    "unpivot = pivot.unstack().unstack().dropna().reset_index()  # values will be in col \"0\"\n",
    "unpivot.loc[:, 1] = unpivot.loc[:, 0].astype(\"str\")\n",
    "unpivot.loc[:, 1] = unpivot.loc[:, 0].apply(lambda x: f\"{round(x, 2):.2f}\")\n",
    "pivot = unpivot.pivot(index=[\"Measure Type\", \"Sim Meas.\"],\n",
    "    columns=col_levels,\n",
    "    values=1,\n",
    ").sort_values(\n",
    "    by=[\"Measure Type\", \"Sim Meas.\"]).reindex(\n",
    "    measure_type_order, axis=\"index\", level=0\n",
    ")\n",
    "unpivot\n",
    "if verbose:\n",
    "    display(unpivot.head(3))\n",
    "\n",
    "# Highlight the best values by bolding\n",
    "for column in pivot.columns:\n",
    "    col = pivot.loc[:, column].astype(\"float\")\n",
    "    idx = col == col.max()\n",
    "    pivot.loc[idx, column] = pivot.loc[idx, column].apply(lambda s: r\"\\textbf{\" + s + \"}\")\n",
    "if verbose:\n",
    "    display(pivot.head(3))\n",
    "\n",
    "\n",
    "# Add significance markers\n",
    "# 1) select data that should get markers\n",
    "idx = data[\"Dataset\"].isin(datasets) & data[\"Arch.\"].isin(archs) & data.Test.isin(tests_with_pvals)\n",
    "data_corr = data.loc[idx].copy()\n",
    "\n",
    "# 2) Create new column with value and marker\n",
    "data_corr[\"val_comb\"] = data_corr[\"value\"].apply(lambda x: f\"{round(x, ndigits=2):.2f}\") + data_corr[\"pval\"].apply(pval_str)\n",
    "if verbose:\n",
    "    display(data_corr.head(3))\n",
    "\n",
    "# 3) Create pivot table for values with markers that can be inserted into the main pivot table\n",
    "pivot_corr = data_corr.pivot(\n",
    "    index=[\"Measure Type\", \"Sim Meas.\"],\n",
    "    columns=col_levels,\n",
    "    values=[\"val_comb\"],\n",
    ").sort_values(\n",
    "    by=[\"Measure Type\", \"Sim Meas.\"],\n",
    ").reindex(\n",
    "    measure_type_order, axis=\"index\", level=0\n",
    ").reindex(\n",
    "    column_order, axis=\"columns\", level=\"Test\"\n",
    ").loc[:, \"val_comb\"]\n",
    "if verbose:\n",
    "    display(pivot_corr.head())\n",
    "\n",
    "# 4) Highlight the best scores by bolding\n",
    "for column in pivot_corr.columns:\n",
    "    col = pivot_corr.loc[:, column].apply(floatify).astype(\"float\")\n",
    "    identifiers = pivot_corr.loc[:, column].apply(separate_significance_indicator)\n",
    "    idx = col == col.max()\n",
    "    new_col = col.apply(lambda x: f\"{x:.2f}\").apply(lambda s: r\"\\textbf{\" + s + \"}\") + identifiers\n",
    "    pivot_corr.loc[idx, column] = new_col\n",
    "\n",
    "# Insert into main pivot\n",
    "pivot.loc[:, (\"Grounding by Prediction\")] = pivot_corr\n",
    "\n",
    "display(pivot.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texify(\n",
    "    pivot.loc[:, (\"Grounding by Prediction\", \"Acc. Corr.\")],\n",
    "    \"tables/graph_test_1.tex\",\n",
    "    r\"Results of Test 1 (\\emph{Correlation to Accuracy Difference}) for the graph domain.\",\n",
    "    \"tab:graph_results_test_1\",\n",
    "    resizebox_width=0.7,\n",
    ")\n",
    "\n",
    "texify(\n",
    "    pivot.loc[:, (\"Grounding by Prediction\", [\"JSD Corr.\", \"Disagr. Corr.\"])],\n",
    "    \"tables/graph_test_2.tex\",\n",
    "    r\"Results of Test 2 (\\emph{Correlation to Output Difference}) for the graph domain.\",\n",
    "    \"tab:graph_results_test_2\",\n",
    ")\n",
    "\n",
    "texify(\n",
    "    pivot.loc[:, (\"Grounding by Design\", \"Random Labels\")],\n",
    "    \"tables/graph_test_3.tex\",\n",
    "    r\"Results of Test 3 (\\emph{Label Randomization}) for the graph domain.\",\n",
    "    \"tab:graph_results_test_3\",\n",
    ")\n",
    "\n",
    "texify(\n",
    "    pivot.loc[:, (\"Grounding by Design\", \"Shortcuts\")],\n",
    "    \"tables/graph_test_4.tex\",\n",
    "    r\"Results of Test 4 (\\emph{Shortcut Affinity}) for the graph domain.\",\n",
    "    \"tab:graph_results_test_4\",\n",
    ")\n",
    "\n",
    "texify(\n",
    "    pivot.loc[:, (\"Grounding by Design\", \"Augmentation\")],\n",
    "    \"tables/graph_test_5.tex\",\n",
    "    r\"Results of Test 5 (\\emph{Augmentation}) for the graph domain.\",\n",
    "    \"tab:graph_results_test_5\",\n",
    ")\n",
    "\n",
    "texify(\n",
    "    pivot.loc[:, (\"Grounding by Design\", \"Layer Mono.\")],\n",
    "    \"tables/graph_test_6.tex\",\n",
    "    r\"Results of Test 6 (\\emph{Layer Monotonicity}) for the graph domain.\",\n",
    "    \"tab:graph_results_test_6\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
